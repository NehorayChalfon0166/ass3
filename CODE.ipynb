{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 0) Setup + robust CSV loading (EDA-safe)\n",
        "import os, re, unicodedata, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "BASE = \"/content\"\n",
        "\n",
        "train_path  = os.path.join(BASE, \"train.csv\")\n",
        "test_path   = os.path.join(BASE, \"test.csv\")\n",
        "desc_path   = os.path.join(BASE, \"product_descriptions.csv\")\n",
        "attr_path   = os.path.join(BASE, \"attributes.csv\")\n",
        "tlabels_path = os.path.join(BASE, \"testLabels.csv\")\n",
        "\n",
        "NROWS = 200_000  # EDA sample size\n",
        "\n",
        "def read_csv_robust(path, nrows=None):\n",
        "    # Prefer \"replace\" to avoid dropping lines fallback to latin1 if needed.\n",
        "    try:\n",
        "        return pd.read_csv(path, nrows=nrows, encoding=\"utf-8\", encoding_errors=\"replace\")\n",
        "    except TypeError:\n",
        "        # older pandas without encoding_errors\n",
        "        return pd.read_csv(path, nrows=nrows, encoding=\"latin1\")\n",
        "    except Exception:\n",
        "        return pd.read_csv(path, nrows=nrows, encoding=\"latin1\", engine=\"python\", on_bad_lines=\"skip\")\n",
        "\n",
        "train = read_csv_robust(train_path, nrows=NROWS)\n",
        "test  = read_csv_robust(test_path,  nrows=NROWS)\n",
        "descs = read_csv_robust(desc_path,  nrows=NROWS)\n",
        "attrs = read_csv_robust(attr_path,  nrows=NROWS)\n",
        "\n",
        "print(\"train:\", train.shape, \"test:\", test.shape)\n",
        "print(\"descs:\", descs.shape, \"attrs:\", attrs.shape)\n",
        "print(\"\\ntrain head:\\n\", train.head(2))\n",
        "print(\"\\ndescs head:\\n\", descs.head(2))\n",
        "\n",
        "# Merge descriptions (needed for query<->description work)\n",
        "train_m = train.merge(descs, on=\"product_uid\", how=\"left\")\n",
        "test_m  = test.merge(descs, on=\"product_uid\", how=\"left\")\n",
        "print(\"\\ntrain_m:\", train_m.shape, \"missing desc rate:\", float(train_m[\"product_description\"].isna().mean()))\n",
        "\n",
        "# 1) Label distribution + basic stats\n",
        "plt.figure()\n",
        "train[\"relevance\"].hist(bins=30)\n",
        "plt.title(\"Relevance distribution (train sample)\")\n",
        "plt.xlabel(\"relevance\"); plt.ylabel(\"count\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nrelevance min/max:\", float(train[\"relevance\"].min()), float(train[\"relevance\"].max()))\n",
        "print(train[\"relevance\"].describe())\n",
        "\n",
        "# 2) Length plots + quantiles\n",
        "def s_len(x):\n",
        "    return 0 if pd.isna(x) else len(str(x))\n",
        "\n",
        "for col in [\"search_term\", \"product_title\", \"product_description\"]:\n",
        "    if col in train_m.columns:\n",
        "        train_m[col+\"_charlen\"] = train_m[col].map(s_len)\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(train_m[\"search_term_charlen\"], bins=50)\n",
        "plt.title(\"search_term char length\")\n",
        "plt.xlabel(\"chars\"); plt.ylabel(\"count\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(train_m[\"product_title_charlen\"], bins=50)\n",
        "plt.title(\"product_title char length\")\n",
        "plt.xlabel(\"chars\"); plt.ylabel(\"count\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(train_m[\"product_description_charlen\"], bins=80)\n",
        "plt.title(\"product_description char length (sample)\")\n",
        "plt.xlabel(\"chars\"); plt.ylabel(\"count\")\n",
        "plt.show()\n",
        "\n",
        "for c in [\"search_term_charlen\",\"product_title_charlen\",\"product_description_charlen\"]:\n",
        "    q = train_m[c].quantile([0.5,0.8,0.9,0.95,0.99]).to_dict()\n",
        "    print(c, q)\n",
        "\n",
        "# 3) Tokenizer for Q2 (word/char-combination tokens)\n",
        "FRACTIONS = \"¼½¾⅓⅔⅛⅜⅝⅞\"\n",
        "token_re = re.compile(\n",
        "    rf\"(#[A-Za-z0-9_]+)|\"          # hashtags\n",
        "    rf\"(\\d+(?:\\.\\d+)?°?)|\"         # numbers, maybe degree\n",
        "    rf\"([{FRACTIONS}])|\"           # unicode fractions\n",
        "    rf\"([A-Za-z]+[A-Za-z0-9_-]*)|\" # words (handles e.g. wire-backed, pull-out)\n",
        "    rf\"([^\\w\\s])\"                  # leftover punctuation/symbols\n",
        ")\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = \"\" if s is None or (isinstance(s, float) and np.isnan(s)) else str(s)\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    return s.lower()\n",
        "\n",
        "def tokenize(s: str):\n",
        "    s = normalize_text(s)\n",
        "    toks = []\n",
        "    for m in token_re.finditer(s):\n",
        "        tok = next(g for g in m.groups() if g is not None)\n",
        "        tok = tok.strip()\n",
        "        if tok:\n",
        "            toks.append(tok)\n",
        "    return toks\n",
        "\n",
        "# Show tokenization examples (good for the report)\n",
        "ex = train_m.sample(8, random_state=1)[[\"search_term\",\"product_title\",\"relevance\"]].copy()\n",
        "for _, r in ex.iterrows():\n",
        "    print(\"\\n---\")\n",
        "    print(\"search_term:\", r[\"search_term\"])\n",
        "    print(\"tokens:\", tokenize(r[\"search_term\"])[:40])\n",
        "    print(\"title:\", r[\"product_title\"])\n",
        "    print(\"tokens:\", tokenize(r[\"product_title\"])[:40])\n",
        "    print(\"relevance:\", r[\"relevance\"])\n",
        "\n",
        "# 4) Overlap/Jaccard quick signal plots\n",
        "def jaccard(a, b):\n",
        "    A, B = set(a), set(b)\n",
        "    return 0.0 if len(A|B)==0 else len(A&B)/len(A|B)\n",
        "\n",
        "def overlap_count(a,b):\n",
        "    A,B=set(a),set(b)\n",
        "    return len(A&B)\n",
        "\n",
        "# tokenize sample columns\n",
        "train_m[\"q_toks\"] = train_m[\"search_term\"].map(tokenize)\n",
        "train_m[\"d_toks\"] = train_m[\"product_description\"].map(tokenize)\n",
        "\n",
        "train_m[\"jacc_q_desc\"] = [jaccard(a,b) for a,b in zip(train_m[\"q_toks\"], train_m[\"d_toks\"])]\n",
        "train_m[\"ovl_q_desc\"]  = [overlap_count(a,b) for a,b in zip(train_m[\"q_toks\"], train_m[\"d_toks\"])]\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(train_m[\"jacc_q_desc\"], train_m[\"relevance\"], s=5, alpha=0.25)\n",
        "plt.title(\"Relevance vs Jaccard(query, description) [sample]\")\n",
        "plt.xlabel(\"jaccard\"); plt.ylabel(\"relevance\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(train_m[\"ovl_q_desc\"], train_m[\"relevance\"], s=5, alpha=0.25)\n",
        "plt.title(\"Relevance vs overlap-count(query, description) [sample]\")\n",
        "plt.xlabel(\"overlap count\"); plt.ylabel(\"relevance\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCorrelation (sample):\")\n",
        "print(train_m[[\"jacc_q_desc\",\"ovl_q_desc\",\"relevance\"]].corr(numeric_only=True))\n",
        "\n",
        "# 5) Find “special tokens” (°, fractions, hashtags) for report examples\n",
        "special_pat = re.compile(r\"[°¼½¾⅓⅔⅛⅜⅝⅞]\")\n",
        "mask = train_m[\"search_term\"].astype(str).str.contains(special_pat)\n",
        "hits = train_m.loc[mask, \"search_term\"]\n",
        "print(\"\\nSpecial-token query hits in sample:\", len(hits))\n",
        "if len(hits):\n",
        "    print(hits.sample(min(20, len(hits)), random_state=0).tolist())\n",
        "\n",
        "# 6) Proper train/val split for modeling (group by product_uid to avoid leakage)\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "df = train_m.dropna(subset=[\"product_description\", \"search_term\", \"product_title\", \"relevance\"]).copy()\n",
        "\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, val_idx = next(gss.split(df, groups=df[\"product_uid\"].values))\n",
        "\n",
        "tr = df.iloc[train_idx].reset_index(drop=True)\n",
        "va = df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "print(\"\\nSplit sizes:\", tr.shape, va.shape)\n",
        "print(\"Shared products (must be 0):\", len(set(tr.product_uid) & set(va.product_uid)))\n",
        "\n",
        "print(\"\\nTrain sample rows:\")\n",
        "print(tr[[\"product_uid\",\"search_term\",\"product_title\",\"relevance\"]].head(2))\n",
        "\n",
        "# 7) Load testLabels (for later evaluation only)\n",
        "test_labels = read_csv_robust(tlabels_path, nrows=None)\n",
        "print(\"\\nTestLabels Usage counts:\")\n",
        "if \"Usage\" in test_labels.columns:\n",
        "    print(test_labels[\"Usage\"].value_counts())\n",
        "else:\n",
        "    print(\"No 'Usage' column found. Columns:\", list(test_labels.columns))\n",
        "\n",
        "# Keep these objects for next steps:\n",
        "# tr, va, test_m, test_labels\n",
        "def eval_feature_extractor_on_test(\n",
        "    model_name,\n",
        "    best_path,\n",
        "    test_m,\n",
        "    test_labels,\n",
        "    batch_size_embed=256,\n",
        "    batch_size_pred=1024\n",
        "):\n",
        "    # Build test texts\n",
        "    test_q = test_m[\"search_term\"].astype(str).tolist()\n",
        "    test_doc = (test_m[\"product_title\"].astype(str) + \" [SEP] \" +\n",
        "                test_m[\"product_description\"].astype(str)).tolist()\n",
        "\n",
        "    # SentenceTransformer\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    st = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "    # Embed\n",
        "    test_q_emb = st.encode(test_q, batch_size=batch_size_embed, show_progress_bar=True,\n",
        "                           convert_to_numpy=True, normalize_embeddings=True)\n",
        "    test_doc_emb = st.encode(test_doc, batch_size=batch_size_embed, show_progress_bar=True,\n",
        "                             convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "    # Features\n",
        "    X_te = build_pair_features(test_q_emb, test_doc_emb)\n",
        "\n",
        "    # Load MLP\n",
        "    mlp = MLPRegressor(X_te.shape[1], dropout=0.0).to(device)\n",
        "    mlp.load_state_dict(torch.load(best_path, map_location=device))\n",
        "    mlp.eval()\n",
        "\n",
        "    # Predict\n",
        "    preds = []\n",
        "    te_loader = DataLoader(torch.tensor(X_te, dtype=torch.float32), batch_size=batch_size_pred, shuffle=False)\n",
        "    with torch.no_grad():\n",
        "        for Xb in te_loader:\n",
        "            preds.append(mlp(Xb.to(device)).cpu().numpy())\n",
        "    preds = np.concatenate(preds)\n",
        "\n",
        "    # Merge with test labels\n",
        "    tl = test_labels.copy()\n",
        "    if \"Usage\" in tl.columns:\n",
        "        tl = tl[tl[\"Usage\"].isin([\"Public\", \"Private\"])].copy()\n",
        "\n",
        "    # Ensure alignment by id\n",
        "    pred_df = pd.DataFrame({\"id\": test_m[\"id\"].values, \"pred\": preds})\n",
        "    merged = tl.merge(pred_df, on=\"id\", how=\"inner\")\n",
        "\n",
        "    y_true = merged[\"relevance\"].astype(np.float32).values\n",
        "    y_pred = merged[\"pred\"].astype(np.float32).values\n",
        "\n",
        "    return rmse(y_true, y_pred), mae(y_true, y_pred), len(merged)\n",
        "\n"
      ],
      "metadata": {
        "id": "qzibxmTO1bYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install wandb gensim\n",
        "\n",
        "import os, re, unicodedata, time, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNYEddhL1bWG",
        "outputId": "fa85e51c-0986-4a19-efb0-05d598b1da22"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0Icwh1w1bUO",
        "outputId": "5fc8a626-3729-401a-d2a3-d33abe036a38"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnavians\u001b[0m (\u001b[33mnavians-ben-gurion-university-of-the-negev\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2c — Word2Vec + Siamese BiLSTM regressor"
      ],
      "metadata": {
        "id": "wHthUCL5YzuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install wandb gensim\n",
        "\n",
        "import os, re, unicodedata, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Q2c — Word2Vec + Siamese BiLSTM regressor\n",
        "\n",
        "\n",
        "# Tokenizer (word / char-combination)\n",
        "FRACTIONS = \"¼½¾⅓⅔⅛⅜⅝⅞\"\n",
        "token_re = re.compile(\n",
        "    rf\"(#[A-Za-z0-9_]+)|\"          # hashtags\n",
        "    rf\"(\\d+(?:\\.\\d+)?°?)|\"         # numbers, maybe degree\n",
        "    rf\"([{FRACTIONS}])|\"           # unicode fractions\n",
        "    rf\"([A-Za-z]+[A-Za-z0-9_-]*)|\" # words (wire-backed / pull-out)\n",
        "    rf\"([^\\w\\s])\"                  # punctuation/symbols\n",
        ")\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = \"\" if s is None or (isinstance(s, float) and np.isnan(s)) else str(s)\n",
        "    return unicodedata.normalize(\"NFKC\", s).lower()\n",
        "\n",
        "def tokenize(s: str):\n",
        "    s = normalize_text(s)\n",
        "    toks = []\n",
        "    for m in token_re.finditer(s):\n",
        "        tok = next(g for g in m.groups() if g is not None).strip()\n",
        "        if tok:\n",
        "            toks.append(tok)\n",
        "    return toks\n",
        "\n",
        "# Hyperparams for truncation\n",
        "Q_MAX_TOK = 16\n",
        "D_MAX_TOK = 256  # model input length for doc tokens\n",
        "\n",
        "USE_FULL_DESCR_FOR_DOC = True\n",
        "DOC_EXTRA_CAP = 1024  # only affects Word2Vec corpus building / token pool\n",
        "\n",
        "def build_pair_tokens(df: pd.DataFrame):\n",
        "    # Query tokens\n",
        "    q = df[\"search_term\"].map(tokenize).map(lambda x: x[:Q_MAX_TOK]).tolist()\n",
        "\n",
        "    # Doc tokens: title + description\n",
        "    doc_text = (df[\"product_title\"].astype(str) + \" [SEP] \" + df[\"product_description\"].astype(str))\n",
        "    max_doc_for_corpus = (DOC_EXTRA_CAP if USE_FULL_DESCR_FOR_DOC else D_MAX_TOK)\n",
        "    d = doc_text.map(tokenize).map(lambda x: x[:max_doc_for_corpus]).tolist()\n",
        "\n",
        "    return q, d\n",
        "\n",
        "# Train Word2Vec ONLY on tr (no leakage)\n",
        "tr_q, tr_d = build_pair_tokens(tr)\n",
        "corpus = tr_q + tr_d\n",
        "\n",
        "W2V_DIM = 200\n",
        "w2v = Word2Vec(\n",
        "    sentences=corpus,\n",
        "    vector_size=W2V_DIM,\n",
        "    window=5,\n",
        "    min_count=2,\n",
        "    workers=2,\n",
        "    sg=1,\n",
        "    negative=10,\n",
        "    epochs=5\n",
        ")\n",
        "\n",
        "PAD = \"<PAD>\"\n",
        "UNK = \"<UNK>\"\n",
        "\n",
        "vocab = [PAD, UNK] + list(w2v.wv.index_to_key)\n",
        "stoi = {t: i for i, t in enumerate(vocab)}\n",
        "pad_idx = stoi[PAD]\n",
        "unk_idx = stoi[UNK]\n",
        "\n",
        "emb = np.zeros((len(vocab), W2V_DIM), dtype=np.float32)\n",
        "emb[unk_idx] = np.random.normal(0, 0.02, size=(W2V_DIM,)).astype(np.float32)\n",
        "for token in w2v.wv.index_to_key:\n",
        "    emb[stoi[token]] = w2v.wv[token]\n",
        "emb_t = torch.tensor(emb)\n",
        "\n",
        "def to_ids(tokens, max_len):\n",
        "    ids = [stoi.get(t, unk_idx) for t in tokens[:max_len]]\n",
        "    if len(ids) < max_len:\n",
        "        ids += [pad_idx] * (max_len - len(ids))\n",
        "    return ids\n",
        "\n",
        "# Dataset\n",
        "class RelevancePairs(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.q_toks, self.d_toks = build_pair_tokens(self.df)\n",
        "        self.y = self.df[\"relevance\"].astype(np.float32).values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        q = to_ids(self.q_toks[idx], Q_MAX_TOK)\n",
        "        d = to_ids(self.d_toks[idx], D_MAX_TOK)  # model sees fixed length here\n",
        "        y = self.y[idx]\n",
        "        return (\n",
        "            torch.tensor(q, dtype=torch.long),\n",
        "            torch.tensor(d, dtype=torch.long),\n",
        "            torch.tensor(y, dtype=torch.float32),\n",
        "        )\n",
        "\n",
        "train_ds = RelevancePairs(tr)\n",
        "val_ds   = RelevancePairs(va)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# Model (pad-aware masking)\n",
        "class SeqEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden=128, num_layers=1, dropout=0.1, pad_idx=0, emb_weights=None):\n",
        "        super().__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        if emb_weights is not None:\n",
        "            self.embedding.weight.data.copy_(emb_weights)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            emb_dim, hidden,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=0.0 if num_layers == 1 else dropout\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e = self.embedding(x)        # [B, T, E]\n",
        "        out, _ = self.lstm(e)        # [B, T, 2H]\n",
        "\n",
        "        mask = (x != self.pad_idx).float().unsqueeze(-1)  # [B, T, 1]\n",
        "        out = out * mask\n",
        "        denom = mask.sum(dim=1).clamp(min=1.0)\n",
        "        pooled = out.sum(dim=1) / denom                   # [B, 2H]\n",
        "        return self.dropout(pooled)\n",
        "\n",
        "class SiameseRegressor(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden=128, dropout=0.2, pad_idx=0, emb_weights=None):\n",
        "        super().__init__()\n",
        "        self.enc = SeqEncoder(\n",
        "            vocab_size, emb_dim,\n",
        "            hidden=hidden,\n",
        "            dropout=dropout,\n",
        "            pad_idx=pad_idx,\n",
        "            emb_weights=emb_weights\n",
        "        )\n",
        "        dim = 2 * hidden\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(dim * 4, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, q, d):\n",
        "        qv = self.enc(q)\n",
        "        dv = self.enc(d)\n",
        "        feats = torch.cat([qv, dv, torch.abs(qv - dv), qv * dv], dim=1)\n",
        "        return self.head(feats).squeeze(1)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SiameseRegressor(\n",
        "    len(vocab), W2V_DIM,\n",
        "    hidden=128,\n",
        "    dropout=0.2,\n",
        "    pad_idx=pad_idx,\n",
        "    emb_weights=emb_t\n",
        ").to(device)\n",
        "\n",
        "# Metrics + eval\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return float(np.mean(np.abs(y_true - y_pred)))\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    for q, d, y in loader:\n",
        "        q, d = q.to(device), d.to(device)\n",
        "        pred = model(q, d).detach().cpu().numpy()\n",
        "        ys.append(y.numpy())\n",
        "        ps.append(pred)\n",
        "    y_true = np.concatenate(ys)\n",
        "    y_pred = np.concatenate(ps)\n",
        "    return rmse(y_true, y_pred), mae(y_true, y_pred)\n",
        "\n",
        "# Correct TEST eval for Siamese model\n",
        "@torch.no_grad()\n",
        "def eval_siamese_on_test(best_path, test_m, test_labels, batch_size=256):\n",
        "    \"\"\"\n",
        "    Evaluate SiameseRegressor on the labeled part of test set.\n",
        "    IMPORTANT: ignores Usage == \"Ignored\" if exists.\n",
        "    Assumes test_m includes product_title + product_description columns (merged).\n",
        "    \"\"\"\n",
        "    df_test = test_m.copy()\n",
        "    df_test[\"relevance\"] = 0.0\n",
        "\n",
        "    test_ds = RelevancePairs(df_test)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    preds = []\n",
        "    ids = df_test[\"id\"].values\n",
        "    for q, d, _ in test_loader:\n",
        "        q, d = q.to(device), d.to(device)\n",
        "        p = model(q, d).detach().cpu().numpy()\n",
        "        preds.append(p)\n",
        "    preds = np.concatenate(preds)\n",
        "\n",
        "    tl = test_labels.copy()\n",
        "    if \"Usage\" in tl.columns:\n",
        "        tl = tl[tl[\"Usage\"].isin([\"Public\", \"Private\"])].copy()\n",
        "\n",
        "    pred_df = pd.DataFrame({\"id\": ids, \"pred\": preds})\n",
        "    merged = tl.merge(pred_df, on=\"id\", how=\"inner\")\n",
        "\n",
        "    y_true = merged[\"relevance\"].astype(np.float32).values\n",
        "    y_pred = merged[\"pred\"].astype(np.float32).values\n",
        "\n",
        "    return rmse(y_true, y_pred), mae(y_true, y_pred), len(merged)\n",
        "\n",
        "# Train with W&B (saves BEST checkpoint)\n",
        "run = wandb.init(\n",
        "    project=\"dl_relevance\",\n",
        "    name=\"Q2_word2vec_siamese_bilstm_fixed\",\n",
        "    config={\n",
        "        \"Q_MAX_TOK\": Q_MAX_TOK,\n",
        "        \"D_MAX_TOK\": D_MAX_TOK,\n",
        "        \"W2V_DIM\": W2V_DIM,\n",
        "        \"hidden\": 128,\n",
        "        \"batch_size\": 256,\n",
        "        \"lr\": 2e-3,\n",
        "        \"epochs\": 6,\n",
        "        \"USE_FULL_DESCR_FOR_DOC\": USE_FULL_DESCR_FOR_DOC,\n",
        "        \"DOC_EXTRA_CAP\": DOC_EXTRA_CAP,\n",
        "    }\n",
        ")\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=run.config.lr)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "best_val_rmse = 1e9\n",
        "best_path = \"/content/best_q2_word_siamese_fixed.pt\"\n",
        "t0 = time.time()\n",
        "\n",
        "for epoch in range(1, run.config.epochs + 1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for step, (q, d, y) in enumerate(train_loader, 1):\n",
        "        q, d, y = q.to(device), d.to(device), y.to(device)\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        pred = model(q, d)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        running_loss += float(loss.item())\n",
        "        if step % 100 == 0:\n",
        "            wandb.log({\"train/loss_step\": running_loss / step, \"epoch\": epoch, \"step\": step})\n",
        "\n",
        "    train_rmse, train_mae = evaluate(model, train_loader)\n",
        "    val_rmse, val_mae     = evaluate(model, val_loader)\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch,\n",
        "        \"train/loss_epoch\": running_loss / max(1, len(train_loader)),\n",
        "        \"train/rmse\": train_rmse,\n",
        "        \"train/mae\": train_mae,\n",
        "        \"val/rmse\": val_rmse,\n",
        "        \"val/mae\": val_mae\n",
        "    })\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | train RMSE {train_rmse:.4f} MAE {train_mae:.4f} | val RMSE {val_rmse:.4f} MAE {val_mae:.4f}\")\n",
        "\n",
        "    if val_rmse < best_val_rmse:\n",
        "        best_val_rmse = val_rmse\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        wandb.save(best_path)\n",
        "        print(\"saved best:\", best_path)\n",
        "\n",
        "runtime_sec = time.time() - t0\n",
        "wandb.log({\"runtime_sec\": runtime_sec})\n",
        "print(\"Total runtime (sec):\", int(runtime_sec))\n",
        "\n",
        "\n",
        "# Final best-checkpoint metrics (Train/Val/Test)\n",
        "model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "train_rmse, train_mae = evaluate(model, train_loader)\n",
        "val_rmse, val_mae     = evaluate(model, val_loader)\n",
        "\n",
        "test_rmse, test_mae, n = eval_siamese_on_test(\n",
        "    best_path=best_path,\n",
        "    test_m=test_m,\n",
        "    test_labels=test_labels,\n",
        "    batch_size=256\n",
        ")\n",
        "\n",
        "print(\"\\nFINAL METRICS (best checkpoint)\")\n",
        "print(f\"Train | RMSE: {train_rmse:.4f} | MAE: {train_mae:.4f}\")\n",
        "print(f\"Val   | RMSE: {val_rmse:.4f} | MAE: {val_mae:.4f}\")\n",
        "print(f\"Test  | RMSE: {test_rmse:.4f} | MAE: {test_mae:.4f} | n={n}\")\n",
        "\n",
        "wandb.log({\n",
        "    \"final/train_rmse\": train_rmse, \"final/train_mae\": train_mae,\n",
        "    \"final/val_rmse\": val_rmse,     \"final/val_mae\": val_mae,\n",
        "    \"final/test_rmse\": test_rmse,   \"final/test_mae\": test_mae,\n",
        "    \"final/test_n\": n\n",
        "})\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "B_2OHDx1ECA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2D\n"
      ],
      "metadata": {
        "id": "l0_6xlvsCfR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2d — Feature extractor (Sentence-Transformers) + MLP regressor\n",
        "\n",
        "\n",
        "!pip -q install sentence-transformers\n",
        "\n",
        "import time, numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Build the \"doc\" text (title + description)\n",
        "def make_doc_text(df):\n",
        "    return (df[\"product_title\"].astype(str) + \" [SEP] \" + df[\"product_description\"].astype(str)).tolist()\n",
        "\n",
        "tr_q = tr[\"search_term\"].astype(str).tolist()\n",
        "va_q = va[\"search_term\"].astype(str).tolist()\n",
        "tr_doc = make_doc_text(tr)\n",
        "va_doc = make_doc_text(va)\n",
        "\n",
        "y_tr = tr[\"relevance\"].astype(np.float32).values\n",
        "y_va = va[\"relevance\"].astype(np.float32).values\n",
        "\n",
        "print(len(tr_q), len(va_q))\n",
        "print(\"Example:\", tr_q[0], \"|\", tr_doc[0][:120], \"...\")\n",
        "\n",
        "def embed_texts(model, texts, batch_size=256):\n",
        "    return model.encode(\n",
        "        texts,\n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True\n",
        "    )\n",
        "\n",
        "def build_pair_features(q_emb, d_emb):\n",
        "    absdiff = np.abs(q_emb - d_emb)\n",
        "    prod    = q_emb * d_emb\n",
        "    cos     = np.sum(q_emb * d_emb, axis=1, keepdims=True)  # normalized => cosine\n",
        "    feats   = np.concatenate([q_emb, d_emb, absdiff, prod, cos], axis=1)\n",
        "    return feats.astype(np.float32)\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return float(np.mean(np.abs(y_true - y_pred)))\n",
        "\n",
        "class FeatDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
        "\n",
        "class MLPRegressor(nn.Module):\n",
        "    def __init__(self, in_dim, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "def run_feature_extractor(\n",
        "    model_name,\n",
        "    batch_size_embed=256,\n",
        "    batch_size_train=512,\n",
        "    epochs=10,\n",
        "    lr=2e-3\n",
        "):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    st = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "    # Embeddings\n",
        "    t0 = time.time()\n",
        "    tr_q_emb   = embed_texts(st, tr_q,   batch_size=batch_size_embed)\n",
        "    tr_doc_emb = embed_texts(st, tr_doc, batch_size=batch_size_embed)\n",
        "    va_q_emb   = embed_texts(st, va_q,   batch_size=batch_size_embed)\n",
        "    va_doc_emb = embed_texts(st, va_doc, batch_size=batch_size_embed)\n",
        "    embed_time = time.time() - t0\n",
        "\n",
        "    X_tr = build_pair_features(tr_q_emb, tr_doc_emb)\n",
        "    X_va = build_pair_features(va_q_emb, va_doc_emb)\n",
        "\n",
        "    train_loader      = DataLoader(FeatDataset(X_tr, y_tr), batch_size=batch_size_train, shuffle=True)\n",
        "    train_loader_eval = DataLoader(FeatDataset(X_tr, y_tr), batch_size=batch_size_train, shuffle=False)\n",
        "    val_loader_eval   = DataLoader(FeatDataset(X_va, y_va), batch_size=batch_size_train, shuffle=False)\n",
        "\n",
        "    model = MLPRegressor(X_tr.shape[1], dropout=0.2).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    run = wandb.init(\n",
        "        project=\"dl_relevance\",\n",
        "        name=f\"Q2d_feat_{model_name.replace('/','_')}_fixed\",\n",
        "        config={\n",
        "            \"model_name\": model_name,\n",
        "            \"embed_batch\": batch_size_embed,\n",
        "            \"train_batch\": batch_size_train,\n",
        "            \"epochs\": epochs,\n",
        "            \"lr\": lr,\n",
        "            \"embed_time_sec\": embed_time,\n",
        "            \"feat_dim\": int(X_tr.shape[1])\n",
        "        }\n",
        "    )\n",
        "\n",
        "    best_val = 1e9\n",
        "    best_path = f\"/content/best_q2d_{model_name.replace('/','_')}_fixed.pt\"\n",
        "    t1 = time.time()\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        # Train\n",
        "        model.train()\n",
        "        total = 0.0\n",
        "        for Xb, yb in train_loader:\n",
        "            Xb, yb = Xb.to(device), yb.to(device)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            pred = model(Xb)\n",
        "            loss = loss_fn(pred, yb)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "            total += float(loss.item())\n",
        "\n",
        "        #Eval (NO shuffle)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            tr_pred = []\n",
        "            for Xb, _ in train_loader_eval:\n",
        "                tr_pred.append(model(Xb.to(device)).cpu().numpy())\n",
        "            va_pred = []\n",
        "            for Xb, _ in val_loader_eval:\n",
        "                va_pred.append(model(Xb.to(device)).cpu().numpy())\n",
        "\n",
        "        tr_pred = np.concatenate(tr_pred)\n",
        "        va_pred = np.concatenate(va_pred)\n",
        "\n",
        "        tr_rmse, tr_mae = rmse(y_tr, tr_pred), mae(y_tr, tr_pred)\n",
        "        va_rmse, va_mae = rmse(y_va, va_pred), mae(y_va, va_pred)\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": ep,\n",
        "            \"train/loss_epoch\": total / max(1, len(train_loader)),\n",
        "            \"train/rmse\": tr_rmse,\n",
        "            \"train/mae\": tr_mae,\n",
        "            \"val/rmse\": va_rmse,\n",
        "            \"val/mae\": va_mae\n",
        "        })\n",
        "\n",
        "        print(f\"{model_name} | Ep {ep:02d} | train RMSE {tr_rmse:.4f} MAE {tr_mae:.4f} | val RMSE {va_rmse:.4f} MAE {va_mae:.4f}\")\n",
        "\n",
        "        if va_rmse < best_val:\n",
        "            best_val = va_rmse\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "            wandb.save(best_path)\n",
        "            print(\" saved best:\", best_path)\n",
        "\n",
        "    train_time = time.time() - t1\n",
        "    total_time = embed_time + train_time\n",
        "    wandb.log({\"runtime_sec\": total_time, \"embed_time_sec\": embed_time, \"train_time_sec\": train_time})\n",
        "\n",
        "    # Load BEST and compute Train/Val final\n",
        "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        tr_pred = []\n",
        "        for Xb, _ in train_loader_eval:\n",
        "            tr_pred.append(model(Xb.to(device)).cpu().numpy())\n",
        "        va_pred = []\n",
        "        for Xb, _ in val_loader_eval:\n",
        "            va_pred.append(model(Xb.to(device)).cpu().numpy())\n",
        "    tr_pred = np.concatenate(tr_pred)\n",
        "    va_pred = np.concatenate(va_pred)\n",
        "\n",
        "    final_tr_rmse, final_tr_mae = rmse(y_tr, tr_pred), mae(y_tr, tr_pred)\n",
        "    final_va_rmse, final_va_mae = rmse(y_va, va_pred), mae(y_va, va_pred)\n",
        "\n",
        "    # TEST\n",
        "    test_rmse, test_mae, n_test = eval_feature_extractor_on_test(\n",
        "        model_name=model_name,\n",
        "        best_path=best_path,\n",
        "        test_m=test_m,\n",
        "        test_labels=test_labels,\n",
        "        batch_size_embed=batch_size_embed\n",
        "    )\n",
        "\n",
        "    wandb.log({\n",
        "        \"best/train_rmse\": final_tr_rmse,\n",
        "        \"best/train_mae\": final_tr_mae,\n",
        "        \"best/val_rmse\": final_va_rmse,\n",
        "        \"best/val_mae\": final_va_mae,\n",
        "        \"test/rmse\": test_rmse,\n",
        "        \"test/mae\": test_mae,\n",
        "        \"test/n_rows\": n_test,\n",
        "    })\n",
        "\n",
        "    print(\"\\nFINAL BEST METRICS\")\n",
        "    print(\"train RMSE:\", final_tr_rmse)\n",
        "    print(\"val   RMSE:\", final_va_rmse)\n",
        "    print(\"test  RMSE:\", test_rmse)\n",
        "    print(\"train MAE :\", final_tr_mae)\n",
        "    print(\"val   MAE :\", final_va_mae)\n",
        "    print(\"test  MAE :\", test_mae)\n",
        "    print(\"runtime_sec:\", int(total_time))\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "    return {\n",
        "        \"model_type\": f\"FeatExt(word) + MLP | {model_name}\",\n",
        "        \"runtime_sec\": float(total_time),\n",
        "        \"train_rmse\": float(final_tr_rmse),\n",
        "        \"val_rmse\": float(final_va_rmse),\n",
        "        \"test_rmse\": float(test_rmse),\n",
        "        \"train_mae\": float(final_tr_mae),\n",
        "        \"val_mae\": float(final_va_mae),\n",
        "        \"test_mae\": float(test_mae),\n",
        "        \"best_path\": best_path\n",
        "    }\n",
        "\n",
        "# Run (2 models)\n",
        "results = []\n",
        "for name in [\n",
        "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    \"sentence-transformers/all-mpnet-base-v2\"\n",
        "]:\n",
        "    results.append(run_feature_extractor(name, epochs=10))\n",
        "\n",
        "results\n"
      ],
      "metadata": {
        "id": "6Y-onlRYNNbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Character-level Siamese (CharCNN+BiLSTM) + Train/Val/Test metrics"
      ],
      "metadata": {
        "id": "SRmM4jw_Lrzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install wandb\n",
        "\n",
        "import os, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "# Character-level Siamese (CharCNN+BiLSTM) + Train/Val/Test metrics\n",
        "\n",
        "\n",
        "# 0) Load + merge (descriptions)\n",
        "BASE = \"/content\"\n",
        "train_path   = os.path.join(BASE, \"train.csv\")\n",
        "test_path    = os.path.join(BASE, \"test.csv\")\n",
        "desc_path    = os.path.join(BASE, \"product_descriptions.csv\")\n",
        "tlabels_path = os.path.join(BASE, \"testLabels.csv\")\n",
        "\n",
        "def read_csv_robust(path):\n",
        "    try:\n",
        "        return pd.read_csv(path, encoding=\"utf-8\", encoding_errors=\"replace\")\n",
        "    except TypeError:\n",
        "        return pd.read_csv(path, encoding=\"latin1\")\n",
        "    except Exception:\n",
        "        return pd.read_csv(path, encoding=\"latin1\", engine=\"python\", on_bad_lines=\"skip\")\n",
        "\n",
        "train = read_csv_robust(train_path)\n",
        "test  = read_csv_robust(test_path)\n",
        "descs = read_csv_robust(desc_path)\n",
        "test_labels = read_csv_robust(tlabels_path)\n",
        "\n",
        "train_m = train.merge(descs, on=\"product_uid\", how=\"left\")\n",
        "test_m  = test.merge(descs, on=\"product_uid\", how=\"left\")\n",
        "\n",
        "# drop missing essentials\n",
        "df = train_m.dropna(subset=[\"search_term\", \"product_title\", \"product_description\", \"relevance\", \"product_uid\"]).copy()\n",
        "\n",
        "# 1) Split tr/va\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, val_idx = next(gss.split(df, groups=df[\"product_uid\"].values))\n",
        "tr = df.iloc[train_idx].reset_index(drop=True)\n",
        "va = df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "print(\"Split sizes:\", tr.shape, va.shape)\n",
        "print(\"Shared products:\", len(set(tr.product_uid) & set(va.product_uid)))\n",
        "\n",
        "# 2) Text builders\n",
        "def make_doc_text(df):\n",
        "    # title + description\n",
        "    return (df[\"product_title\"].astype(str) + \" [SEP] \" + df[\"product_description\"].astype(str)).tolist()\n",
        "\n",
        "tr_q = tr[\"search_term\"].astype(str).tolist()\n",
        "va_q = va[\"search_term\"].astype(str).tolist()\n",
        "te_q = test_m[\"search_term\"].astype(str).tolist()\n",
        "\n",
        "tr_doc = make_doc_text(tr)\n",
        "va_doc = make_doc_text(va)\n",
        "te_doc = make_doc_text(test_m)\n",
        "\n",
        "y_tr = tr[\"relevance\"].astype(np.float32).values\n",
        "y_va = va[\"relevance\"].astype(np.float32).values\n",
        "\n",
        "# 3) Build char vocab on TRAIN ONLY\n",
        "def build_char_vocab(texts, min_freq=2):\n",
        "    from collections import Counter\n",
        "    c = Counter()\n",
        "    for s in texts:\n",
        "        c.update(list(s))\n",
        "    chars = sorted([ch for ch, f in c.items() if f >= min_freq])\n",
        "    PAD, UNK = \"<PAD>\", \"<UNK>\"\n",
        "    itos = [PAD, UNK] + chars\n",
        "    stoi = {ch: i for i, ch in enumerate(itos)}\n",
        "    return stoi, itos\n",
        "\n",
        "stoi, itos = build_char_vocab(tr_q + tr_doc, min_freq=2)\n",
        "PAD_IDX = stoi[\"<PAD>\"]\n",
        "UNK_IDX = stoi[\"<UNK>\"]\n",
        "print(\"Char vocab size:\", len(itos))\n",
        "\n",
        "# 4) Char dataset\n",
        "Q_MAX_CH = 64\n",
        "D_MAX_CH = 1500\n",
        "\n",
        "def to_char_ids(s, max_len):\n",
        "    s = str(s)[:max_len]\n",
        "    ids = [stoi.get(ch, UNK_IDX) for ch in s]\n",
        "    if len(ids) < max_len:\n",
        "        ids += [PAD_IDX] * (max_len - len(ids))\n",
        "    return ids\n",
        "\n",
        "class CharPairs(Dataset):\n",
        "    def __init__(self, q_texts, d_texts, y=None):\n",
        "        self.q = q_texts\n",
        "        self.d = d_texts\n",
        "        self.y = None if y is None else y.astype(np.float32)\n",
        "\n",
        "    def __len__(self): return len(self.q)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        q_ids = torch.tensor(to_char_ids(self.q[i], Q_MAX_CH), dtype=torch.long)\n",
        "        d_ids = torch.tensor(to_char_ids(self.d[i], D_MAX_CH), dtype=torch.long)\n",
        "        if self.y is None:\n",
        "            return q_ids, d_ids\n",
        "        return q_ids, d_ids, torch.tensor(self.y[i], dtype=torch.float32)\n",
        "\n",
        "train_ds = CharPairs(tr_q, tr_doc, y_tr)\n",
        "val_ds   = CharPairs(va_q, va_doc, y_va)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# 5) Model\n",
        "class CharEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=64, conv_channels=128, lstm_hidden=128, dropout=0.2, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(emb_dim, conv_channels, kernel_size=3, padding=1)\n",
        "        self.conv5 = nn.Conv1d(emb_dim, conv_channels, kernel_size=5, padding=2)\n",
        "        self.conv7 = nn.Conv1d(emb_dim, conv_channels, kernel_size=7, padding=3)\n",
        "\n",
        "        self.act = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=conv_channels * 3,\n",
        "            hidden_size=lstm_hidden,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            num_layers=1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        e = self.emb(x)           # [B, T, E]\n",
        "        e = e.transpose(1, 2)     # [B, E, T]\n",
        "\n",
        "        c3 = self.act(self.conv3(e))\n",
        "        c5 = self.act(self.conv5(e))\n",
        "        c7 = self.act(self.conv7(e))\n",
        "\n",
        "        c = torch.cat([c3, c5, c7], dim=1)  # [B, 3C, T]\n",
        "        c = self.dropout(c)\n",
        "        c = c.transpose(1, 2)               # [B, T, 3C]\n",
        "\n",
        "        out, _ = self.lstm(c)               # [B, T, 2H]\n",
        "\n",
        "        mask = (x != PAD_IDX).float().unsqueeze(-1)\n",
        "        out = out * mask\n",
        "        denom = mask.sum(dim=1).clamp(min=1.0)\n",
        "        pooled = out.sum(dim=1) / denom\n",
        "        return self.dropout(pooled)\n",
        "\n",
        "class SiameseCharRegressor(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=64, conv_channels=128, lstm_hidden=128, dropout=0.25):\n",
        "        super().__init__()\n",
        "        self.enc = CharEncoder(vocab_size, emb_dim, conv_channels, lstm_hidden, dropout, pad_idx=PAD_IDX)\n",
        "        dim = 2 * lstm_hidden\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(dim * 4, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, q, d):\n",
        "        qv = self.enc(q)\n",
        "        dv = self.enc(d)\n",
        "        feats = torch.cat([qv, dv, torch.abs(qv - dv), qv * dv], dim=1)\n",
        "        return self.head(feats).squeeze(1)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SiameseCharRegressor(len(itos)).to(device)\n",
        "\n",
        "# 6) Metrics + eval\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return float(np.mean(np.abs(y_true - y_pred)))\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model(model, loader):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    for q, d, y in loader:\n",
        "        q, d = q.to(device), d.to(device)\n",
        "        pred = model(q, d).detach().cpu().numpy()\n",
        "        ys.append(y.numpy())\n",
        "        ps.append(pred)\n",
        "    y_true = np.concatenate(ys)\n",
        "    y_pred = np.concatenate(ps)\n",
        "    return rmse(y_true, y_pred), mae(y_true, y_pred)\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_test(model, te_q_texts, te_doc_texts, batch_size=128):\n",
        "    model.eval()\n",
        "    te_ds = CharPairs(te_q_texts, te_doc_texts, y=None)\n",
        "    te_loader = DataLoader(te_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    preds = []\n",
        "    for q, d in te_loader:\n",
        "        q, d = q.to(device), d.to(device)\n",
        "        preds.append(model(q, d).detach().cpu().numpy())\n",
        "    return np.concatenate(preds)\n",
        "\n",
        "def eval_on_testlabels_char(model, test_m, test_labels):\n",
        "    preds = predict_test(model, te_q, te_doc, batch_size=128)\n",
        "    pred_df = pd.DataFrame({\"id\": test_m[\"id\"].values, \"pred\": preds})\n",
        "\n",
        "    tl = test_labels.copy()\n",
        "    if \"Usage\" in tl.columns:\n",
        "        tl = tl[tl[\"Usage\"].isin([\"Public\", \"Private\"])].copy()\n",
        "\n",
        "    merged = tl.merge(pred_df, on=\"id\", how=\"inner\")\n",
        "    y_true = merged[\"relevance\"].astype(np.float32).values\n",
        "    y_pred = merged[\"pred\"].astype(np.float32).values\n",
        "\n",
        "    return rmse(y_true, y_pred), mae(y_true, y_pred), len(merged)\n",
        "\n",
        "# 7) Train with W&B\n",
        "run = wandb.init(\n",
        "    project=\"dl_relevance\",\n",
        "    name=\"Q1_char_siamese_charcnn_bilstm\",\n",
        "    config={\n",
        "        \"Q_MAX_CH\": Q_MAX_CH,\n",
        "        \"D_MAX_CH\": D_MAX_CH,\n",
        "        \"batch\": 128,\n",
        "        \"lr\": 2e-3,\n",
        "        \"epochs\": 6\n",
        "    }\n",
        ")\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=run.config.lr)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "best_val = 1e9\n",
        "best_path = \"/content/best_char_siamese.pt\"\n",
        "t0 = time.time()\n",
        "\n",
        "for ep in range(1, run.config.epochs + 1):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "\n",
        "    for step, (q, d, y) in enumerate(train_loader, 1):\n",
        "        q, d, y = q.to(device), d.to(device), y.to(device)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        pred = model(q, d)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "        total += float(loss.item())\n",
        "\n",
        "        if step % 200 == 0:\n",
        "            wandb.log({\"train/loss_step\": total / step, \"epoch\": ep, \"step\": step})\n",
        "\n",
        "    tr_rmse, tr_mae = eval_model(model, train_loader)\n",
        "    va_rmse, va_mae = eval_model(model, val_loader)\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": ep,\n",
        "        \"train/loss_epoch\": total / max(1, len(train_loader)),\n",
        "        \"train/rmse\": tr_rmse, \"train/mae\": tr_mae,\n",
        "        \"val/rmse\": va_rmse,   \"val/mae\": va_mae\n",
        "    })\n",
        "\n",
        "    print(f\"Ep {ep:02d} | train RMSE {tr_rmse:.4f} MAE {tr_mae:.4f} | val RMSE {va_rmse:.4f} MAE {va_mae:.4f}\")\n",
        "\n",
        "    if va_rmse < best_val:\n",
        "        best_val = va_rmse\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        wandb.save(best_path)\n",
        "        print(\"saved best:\", best_path)\n",
        "\n",
        "runtime = time.time() - t0\n",
        "wandb.log({\"runtime_sec\": runtime})\n",
        "\n",
        "# 8) FINAL: load best + compute Train/Val/Test (for Q3 table)\n",
        "model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "\n",
        "final_tr_rmse, final_tr_mae = eval_model(model, train_loader)\n",
        "final_va_rmse, final_va_mae = eval_model(model, val_loader)\n",
        "test_rmse, test_mae, n_test = eval_on_testlabels_char(model, test_m, test_labels)\n",
        "\n",
        "print(\"\\nFINAL BEST METRICS (Char Siamese)\")\n",
        "print(f\"Train RMSE: {final_tr_rmse:.4f} | Train MAE: {final_tr_mae:.4f}\")\n",
        "print(f\"Val   RMSE: {final_va_rmse:.4f} | Val   MAE: {final_va_mae:.4f}\")\n",
        "print(f\"Test  RMSE: {test_rmse:.4f} | Test  MAE: {test_mae:.4f} | n={n_test}\")\n",
        "print(\"runtime_sec:\", int(runtime))\n",
        "\n",
        "wandb.log({\n",
        "    \"best/train_rmse\": final_tr_rmse,\n",
        "    \"best/train_mae\": final_tr_mae,\n",
        "    \"best/val_rmse\": final_va_rmse,\n",
        "    \"best/val_mae\": final_va_mae,\n",
        "    \"test/rmse\": test_rmse,\n",
        "    \"test/mae\": test_mae,\n",
        "    \"test/n_rows\": n_test,\n",
        "})\n",
        "wandb.finish()\n",
        "\n",
        "# Row for Q3 table\n",
        "char_siamese_row = {\n",
        "    \"Model type\": \"Character-level Siamese (CharCNN+BiLSTM)\",\n",
        "    \"runtime\": int(runtime),\n",
        "    \"Train RMSE\": float(final_tr_rmse),\n",
        "    \"Val-RMSE\": float(final_va_rmse),\n",
        "    \"Test-RMSE\": float(test_rmse),\n",
        "    \"Train MAE\": float(final_tr_mae),\n",
        "    \"Val-MAE\": float(final_va_mae),\n",
        "    \"Test-MAE\": float(test_mae),\n",
        "    \"best_path\": best_path\n",
        "}\n",
        "char_siamese_row\n"
      ],
      "metadata": {
        "id": "d2xSVuhRPo6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Char feature extractors"
      ],
      "metadata": {
        "id": "wdCeGGrKMmAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers accelerate\n",
        "\n",
        "import os, time, numpy as np, pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb\n",
        "from transformers import AutoTokenizer, AutoModel, T5EncoderModel\n",
        "\n",
        "# Build texts\n",
        "def make_doc_text(df):\n",
        "    return (df[\"product_title\"].astype(str) + \" [SEP] \" + df[\"product_description\"].astype(str)).tolist()\n",
        "\n",
        "tr_q   = tr[\"search_term\"].astype(str).tolist()\n",
        "va_q   = va[\"search_term\"].astype(str).tolist()\n",
        "tr_doc = make_doc_text(tr)\n",
        "va_doc = make_doc_text(va)\n",
        "\n",
        "y_tr = tr[\"relevance\"].astype(np.float32).values\n",
        "y_va = va[\"relevance\"].astype(np.float32).values\n",
        "\n",
        "print(\"Train/Val:\", len(tr_q), len(va_q))\n",
        "\n",
        "# Encoder loading + pooling\n",
        "def mean_pool(last_hidden, attn_mask):\n",
        "    mask = attn_mask.unsqueeze(-1).float()\n",
        "    x = last_hidden * mask\n",
        "    return x.sum(dim=1) / mask.sum(dim=1).clamp(min=1e-6)\n",
        "\n",
        "def load_encoder(model_name, device):\n",
        "    if \"byt5\" in model_name.lower():\n",
        "        tok = AutoTokenizer.from_pretrained(model_name)\n",
        "        mdl = T5EncoderModel.from_pretrained(model_name).to(device)\n",
        "        return tok, mdl\n",
        "    else:\n",
        "        tok = AutoTokenizer.from_pretrained(model_name)\n",
        "        mdl = AutoModel.from_pretrained(model_name).to(device)\n",
        "        return tok, mdl\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_texts(model_name, texts, max_len=256, batch_size=32, cache_path=None):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    if cache_path and os.path.exists(cache_path):\n",
        "        return np.load(cache_path)\n",
        "\n",
        "    tok, mdl = load_encoder(model_name, device)\n",
        "    mdl.eval()\n",
        "\n",
        "    outs = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        enc = tok(batch, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "        out = mdl(**enc)\n",
        "        pooled = mean_pool(out.last_hidden_state, enc[\"attention_mask\"])\n",
        "        pooled = torch.nn.functional.normalize(pooled, dim=1)\n",
        "        outs.append(pooled.cpu().numpy())\n",
        "\n",
        "    arr = np.concatenate(outs, axis=0).astype(np.float32)\n",
        "    if cache_path:\n",
        "        np.save(cache_path, arr)\n",
        "    return arr\n",
        "\n",
        "def build_pair_features(q_emb, d_emb):\n",
        "    absdiff = np.abs(q_emb - d_emb)\n",
        "    prod    = q_emb * d_emb\n",
        "    cos     = np.sum(q_emb * d_emb, axis=1, keepdims=True)  # because normalized\n",
        "    return np.concatenate([q_emb, d_emb, absdiff, prod, cos], axis=1).astype(np.float32)\n",
        "\n",
        "# Dataset + MLP\n",
        "class FeatDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
        "\n",
        "class MLPRegressor(nn.Module):\n",
        "    def __init__(self, in_dim, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x).squeeze(1)\n",
        "\n",
        "def rmse(y, p): return float(np.sqrt(np.mean((y - p) ** 2)))\n",
        "def mae(y, p):  return float(np.mean(np.abs(y - p)))\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_mlp(model, loader, device):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    for Xb, _ in loader:\n",
        "        preds.append(model(Xb.to(device)).cpu().numpy())\n",
        "    return np.concatenate(preds)\n",
        "\n",
        "# Test evaluation\n",
        "def eval_feature_model_on_test(model_name, best_path, max_len_q, max_len_d, embed_bs, test_m, test_labels):\n",
        "    te_q = test_m[\"search_term\"].astype(str).tolist()\n",
        "    te_doc = (test_m[\"product_title\"].astype(str) + \" [SEP] \" + test_m[\"product_description\"].astype(str)).tolist()\n",
        "\n",
        "    safe = model_name.replace(\"/\", \"_\")\n",
        "    qte_path = f\"/content/cache_{safe}_qte.npy\"\n",
        "    dte_path = f\"/content/cache_{safe}_dte.npy\"\n",
        "\n",
        "    qte = encode_texts(model_name, te_q,   max_len=max_len_q, batch_size=embed_bs, cache_path=qte_path)\n",
        "    dte = encode_texts(model_name, te_doc, max_len=max_len_d, batch_size=embed_bs, cache_path=dte_path)\n",
        "    Xte = build_pair_features(qte, dte)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    mlp = MLPRegressor(Xte.shape[1], dropout=0.0).to(device)\n",
        "    mlp.load_state_dict(torch.load(best_path, map_location=device))\n",
        "    mlp.eval()\n",
        "\n",
        "    te_loader = DataLoader(FeatDataset(Xte, np.zeros(len(Xte), dtype=np.float32)),\n",
        "                           batch_size=1024, shuffle=False)\n",
        "\n",
        "    preds = predict_mlp(mlp, te_loader, device)\n",
        "\n",
        "    pred_df = pd.DataFrame({\"id\": test_m[\"id\"].values, \"pred\": preds})\n",
        "    tl = test_labels.copy()\n",
        "    if \"Usage\" in tl.columns:\n",
        "        tl = tl[tl[\"Usage\"].isin([\"Public\", \"Private\"])].copy()\n",
        "\n",
        "    merged = tl.merge(pred_df, on=\"id\", how=\"inner\")\n",
        "    y_true = merged[\"relevance\"].astype(np.float32).values\n",
        "    y_pred = merged[\"pred\"].astype(np.float32).values\n",
        "    return rmse(y_true, y_pred), mae(y_true, y_pred), len(merged)\n",
        "\n",
        "# Main runner\n",
        "def run_char_feature_model(\n",
        "    model_name,\n",
        "    max_len_q=64,\n",
        "    max_len_d=256,\n",
        "    embed_bs=32,\n",
        "    train_bs=512,\n",
        "    epochs=8,\n",
        "    lr=2e-3,\n",
        "    compute_test=True,\n",
        "):\n",
        "    safe = model_name.replace(\"/\", \"_\")\n",
        "\n",
        "    # cache paths\n",
        "    qtr_path = f\"/content/cache_{safe}_qtr.npy\"\n",
        "    dtr_path = f\"/content/cache_{safe}_dtr.npy\"\n",
        "    qva_path = f\"/content/cache_{safe}_qva.npy\"\n",
        "    dva_path = f\"/content/cache_{safe}_dva.npy\"\n",
        "\n",
        "    # embeddings\n",
        "    t0 = time.time()\n",
        "    qtr = encode_texts(model_name, tr_q,   max_len=max_len_q, batch_size=embed_bs, cache_path=qtr_path)\n",
        "    dtr = encode_texts(model_name, tr_doc, max_len=max_len_d, batch_size=embed_bs, cache_path=dtr_path)\n",
        "    qva = encode_texts(model_name, va_q,   max_len=max_len_q, batch_size=embed_bs, cache_path=qva_path)\n",
        "    dva = encode_texts(model_name, va_doc, max_len=max_len_d, batch_size=embed_bs, cache_path=dva_path)\n",
        "    embed_time = time.time() - t0\n",
        "\n",
        "    Xtr = build_pair_features(qtr, dtr)\n",
        "    Xva = build_pair_features(qva, dva)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = MLPRegressor(Xtr.shape[1], dropout=0.2).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    run = wandb.init(\n",
        "        project=\"dl_relevance\",\n",
        "        name=f\"Q_char_feat_{safe}_fixed\",\n",
        "        config={\n",
        "            \"model_name\": model_name,\n",
        "            \"max_len_q\": max_len_q,\n",
        "            \"max_len_d\": max_len_d,\n",
        "            \"embed_bs\": embed_bs,\n",
        "            \"train_bs\": train_bs,\n",
        "            \"epochs\": epochs,\n",
        "            \"lr\": lr,\n",
        "            \"embed_time_sec\": embed_time,\n",
        "            \"feat_dim\": int(Xtr.shape[1]),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    train_loader      = DataLoader(FeatDataset(Xtr, y_tr), batch_size=train_bs, shuffle=True)\n",
        "    train_loader_eval = DataLoader(FeatDataset(Xtr, y_tr), batch_size=train_bs, shuffle=False)\n",
        "    val_loader_eval   = DataLoader(FeatDataset(Xva, y_va), batch_size=train_bs, shuffle=False)\n",
        "\n",
        "    best_val_rmse = 1e9\n",
        "    best_path = f\"/content/best_char_feat_{safe}_fixed.pt\"\n",
        "    t1 = time.time()\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total = 0.0\n",
        "        for Xb, yb in train_loader:\n",
        "            Xb, yb = Xb.to(device), yb.to(device)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            pred = model(Xb)\n",
        "            loss = loss_fn(pred, yb)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "            total += float(loss.item())\n",
        "\n",
        "        # aligned metrics (no shuffle)\n",
        "        tr_pred = predict_mlp(model, train_loader_eval, device)\n",
        "        va_pred = predict_mlp(model, val_loader_eval, device)\n",
        "\n",
        "        tr_rmse, tr_mae = rmse(y_tr, tr_pred), mae(y_tr, tr_pred)\n",
        "        va_rmse, va_mae = rmse(y_va, va_pred), mae(y_va, va_pred)\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": ep,\n",
        "            \"train/loss_epoch\": total / max(1, len(train_loader)),\n",
        "            \"train/rmse\": tr_rmse, \"train/mae\": tr_mae,\n",
        "            \"val/rmse\": va_rmse, \"val/mae\": va_mae\n",
        "        })\n",
        "\n",
        "        print(f\"{model_name} | Ep {ep:02d} | train RMSE {tr_rmse:.4f} MAE {tr_mae:.4f} | val RMSE {va_rmse:.4f} MAE {va_mae:.4f}\")\n",
        "\n",
        "        if va_rmse < best_val_rmse:\n",
        "            best_val_rmse = va_rmse\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "            wandb.save(best_path)\n",
        "            print(\"saved best:\", best_path)\n",
        "\n",
        "    train_time = time.time() - t1\n",
        "    total_time = embed_time + train_time\n",
        "    wandb.log({\"runtime_sec\": total_time, \"embed_time_sec\": embed_time, \"train_time_sec\": train_time})\n",
        "\n",
        "    # ---- load best + final metrics ----\n",
        "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "    tr_pred = predict_mlp(model, train_loader_eval, device)\n",
        "    va_pred = predict_mlp(model, val_loader_eval, device)\n",
        "    final_tr_rmse, final_tr_mae = rmse(y_tr, tr_pred), mae(y_tr, tr_pred)\n",
        "    final_va_rmse, final_va_mae = rmse(y_va, va_pred), mae(y_va, va_pred)\n",
        "\n",
        "    test_rmse = test_mae = None\n",
        "    n_test = 0\n",
        "    if compute_test:\n",
        "        test_rmse, test_mae, n_test = eval_feature_model_on_test(\n",
        "            model_name, best_path, max_len_q, max_len_d, embed_bs, test_m, test_labels\n",
        "        )\n",
        "\n",
        "    print(\"\\nFINAL BEST METRICS:\", model_name)\n",
        "    print(\"train RMSE:\", final_tr_rmse, \"| train MAE:\", final_tr_mae)\n",
        "    print(\"val   RMSE:\", final_va_rmse, \"| val   MAE:\", final_va_mae)\n",
        "    if compute_test:\n",
        "        print(\"test  RMSE:\", test_rmse, \"| test  MAE:\", test_mae, \"| rows:\", n_test)\n",
        "    print(\"runtime_sec:\", int(total_time))\n",
        "\n",
        "    wandb.log({\n",
        "        \"best/train_rmse\": final_tr_rmse,\n",
        "        \"best/train_mae\": final_tr_mae,\n",
        "        \"best/val_rmse\": final_va_rmse,\n",
        "        \"best/val_mae\": final_va_mae,\n",
        "        \"best_path\": best_path\n",
        "    })\n",
        "    if compute_test:\n",
        "        wandb.log({\"test/rmse\": test_rmse, \"test/mae\": test_mae, \"test/n_rows\": n_test})\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "    row = {\n",
        "        \"Model type\": f\"Char feature extractor: {model_name}\",\n",
        "        \"runtime\": int(total_time),\n",
        "        \"Train RMSE\": float(final_tr_rmse),\n",
        "        \"Val-RMSE\": float(final_va_rmse),\n",
        "        \"Test-RMSE\": None if test_rmse is None else float(test_rmse),\n",
        "        \"Train MAE\": float(final_tr_mae),\n",
        "        \"Val-MAE\": float(final_va_mae),\n",
        "        \"Test-MAE\": None if test_mae is None else float(test_mae),\n",
        "        \"best_path\": best_path,\n",
        "        \"max_len_q\": max_len_q,\n",
        "        \"max_len_d\": max_len_d\n",
        "    }\n",
        "    return row\n",
        "\n",
        "# Run 3 models\n",
        "\n",
        "models_to_run = [\n",
        "    (\"google/byt5-small\", 64, 256),\n",
        "    (\"google/byt5-base\",  64, 256),\n",
        "    (\"google/canine-s\",   64, 256),\n",
        "]\n",
        "\n",
        "char_feat_rows = []\n",
        "for name, mq, md in models_to_run:\n",
        "    row = run_char_feature_model(\n",
        "        name,\n",
        "        max_len_q=mq,\n",
        "        max_len_d=md,\n",
        "        embed_bs=32 if \"base\" not in name else 16,\n",
        "        train_bs=512,\n",
        "        epochs=8,\n",
        "        compute_test=True\n",
        "    )\n",
        "    char_feat_rows.append(row)\n",
        "\n",
        "char_feat_rows\n"
      ],
      "metadata": {
        "id": "iPL_hhRvQXMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word feature extractor deberta-v3-base:"
      ],
      "metadata": {
        "id": "cC45wB-g_6Bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# WORD Feature Extractor #1 (strong): DeBERTa-v3-base + MLP regressor\n",
        "\n",
        "!pip -q install transformers accelerate wandb\n",
        "\n",
        "import os, time, numpy as np, pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# -------------------------\n",
        "# Text builders\n",
        "# -------------------------\n",
        "def make_doc_text(df):\n",
        "    return (df[\"product_title\"].astype(str) + \" [SEP] \" + df[\"product_description\"].astype(str)).tolist()\n",
        "\n",
        "tr_q   = tr[\"search_term\"].astype(str).tolist()\n",
        "va_q   = va[\"search_term\"].astype(str).tolist()\n",
        "tr_doc = make_doc_text(tr)\n",
        "va_doc = make_doc_text(va)\n",
        "\n",
        "y_tr = tr[\"relevance\"].astype(np.float32).values\n",
        "y_va = va[\"relevance\"].astype(np.float32).values\n",
        "\n",
        "print(\"Train/Val:\", len(tr_q), len(va_q))\n",
        "print(\"Example:\", tr_q[0], \"|\", tr_doc[0][:90], \"...\")\n",
        "\n",
        "# Encoder (word-level) + pooling\n",
        "MODEL_NAME = \"microsoft/deberta-v3-base\"  # strong + usually stable to run\n",
        "SAFE = MODEL_NAME.replace(\"/\", \"_\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "enc_model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
        "enc_model.eval()\n",
        "\n",
        "def mean_pool(last_hidden, attn_mask):\n",
        "    # last_hidden: [B, T, H], attn_mask: [B, T]\n",
        "    mask = attn_mask.unsqueeze(-1).float()\n",
        "    x = last_hidden * mask\n",
        "    return x.sum(dim=1) / mask.sum(dim=1).clamp(min=1e-6)\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_texts_cached(texts, max_len, batch_size, cache_path):\n",
        "    if cache_path and os.path.exists(cache_path):\n",
        "        return np.load(cache_path)\n",
        "\n",
        "    outs = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        batch_tok = tok(\n",
        "            batch,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        batch_tok = {k: v.to(device) for k, v in batch_tok.items()}\n",
        "\n",
        "        out = enc_model(**batch_tok)\n",
        "        pooled = mean_pool(out.last_hidden_state, batch_tok[\"attention_mask\"])\n",
        "        pooled = torch.nn.functional.normalize(pooled, dim=1)\n",
        "\n",
        "        outs.append(pooled.detach().cpu().numpy())\n",
        "\n",
        "    arr = np.concatenate(outs, axis=0).astype(np.float32)\n",
        "    if cache_path:\n",
        "        np.save(cache_path, arr)\n",
        "    return arr\n",
        "\n",
        "def build_pair_features(q_emb, d_emb):\n",
        "    # assumes q_emb,d_emb are L2-normalized => dot = cosine\n",
        "    absdiff = np.abs(q_emb - d_emb)\n",
        "    prod    = q_emb * d_emb\n",
        "    cos     = np.sum(q_emb * d_emb, axis=1, keepdims=True)\n",
        "    return np.concatenate([q_emb, d_emb, absdiff, prod, cos], axis=1).astype(np.float32)\n",
        "\n",
        "# MLP + helpers\n",
        "class FeatDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
        "\n",
        "class MLPRegressor(nn.Module):\n",
        "    def __init__(self, in_dim, dropout=0.25):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 1024), nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(1024, 256),    nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(256, 64),      nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return float(np.mean(np.abs(y_true - y_pred)))\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_mlp(model, loader, device):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    for Xb, _ in loader:\n",
        "        preds.append(model(Xb.to(device)).cpu().numpy())\n",
        "    return np.concatenate(preds)\n",
        "\n",
        "# Hyperparams\n",
        "MAX_LEN_Q = 32\n",
        "MAX_LEN_D = 256\n",
        "EMBED_BS  = 32\n",
        "TRAIN_BS  = 512\n",
        "EPOCHS    = 10\n",
        "LR        = 2e-3\n",
        "\n",
        "# Build cached embeddings + features\n",
        "t0 = time.time()\n",
        "\n",
        "qtr_path = f\"/content/cache_{SAFE}_qtr_len{MAX_LEN_Q}.npy\"\n",
        "dtr_path = f\"/content/cache_{SAFE}_dtr_len{MAX_LEN_D}.npy\"\n",
        "qva_path = f\"/content/cache_{SAFE}_qva_len{MAX_LEN_Q}.npy\"\n",
        "dva_path = f\"/content/cache_{SAFE}_dva_len{MAX_LEN_D}.npy\"\n",
        "\n",
        "qtr = encode_texts_cached(tr_q,   max_len=MAX_LEN_Q, batch_size=EMBED_BS, cache_path=qtr_path)\n",
        "dtr = encode_texts_cached(tr_doc, max_len=MAX_LEN_D, batch_size=EMBED_BS, cache_path=dtr_path)\n",
        "qva = encode_texts_cached(va_q,   max_len=MAX_LEN_Q, batch_size=EMBED_BS, cache_path=qva_path)\n",
        "dva = encode_texts_cached(va_doc, max_len=MAX_LEN_D, batch_size=EMBED_BS, cache_path=dva_path)\n",
        "\n",
        "embed_time = time.time() - t0\n",
        "print(\"Embed time (sec):\", int(embed_time), \"| dim:\", qtr.shape[1])\n",
        "\n",
        "Xtr = build_pair_features(qtr, dtr)\n",
        "Xva = build_pair_features(qva, dva)\n",
        "print(\"Feature shape:\", Xtr.shape)\n",
        "\n",
        "\n",
        "# Train MLP (best by Val RMSE)\n",
        "device_t = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "mlp = MLPRegressor(Xtr.shape[1], dropout=0.25).to(device_t)\n",
        "opt = torch.optim.AdamW(mlp.parameters(), lr=LR)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "train_loader      = DataLoader(FeatDataset(Xtr, y_tr), batch_size=TRAIN_BS, shuffle=True)\n",
        "train_loader_eval = DataLoader(FeatDataset(Xtr, y_tr), batch_size=TRAIN_BS, shuffle=False)  # aligned\n",
        "val_loader_eval   = DataLoader(FeatDataset(Xva, y_va), batch_size=TRAIN_BS, shuffle=False)\n",
        "\n",
        "run = wandb.init(\n",
        "    project=\"dl_relevance\",\n",
        "    name=f\"WORD_FE_{SAFE}_mlp\",\n",
        "    config={\n",
        "        \"model_name\": MODEL_NAME,\n",
        "        \"MAX_LEN_Q\": MAX_LEN_Q,\n",
        "        \"MAX_LEN_D\": MAX_LEN_D,\n",
        "        \"EMBED_BS\": EMBED_BS,\n",
        "        \"TRAIN_BS\": TRAIN_BS,\n",
        "        \"EPOCHS\": EPOCHS,\n",
        "        \"LR\": LR,\n",
        "        \"embed_time_sec\": embed_time,\n",
        "        \"feat_dim\": int(Xtr.shape[1]),\n",
        "    }\n",
        ")\n",
        "\n",
        "best_val = 1e9\n",
        "best_path = f\"/content/best_word_feat_{SAFE}.pt\"\n",
        "t1 = time.time()\n",
        "\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    mlp.train()\n",
        "    total = 0.0\n",
        "    for Xb, yb in train_loader:\n",
        "        Xb, yb = Xb.to(device_t), yb.to(device_t)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        pred = mlp(Xb)\n",
        "        loss = loss_fn(pred, yb)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(mlp.parameters(), 1.0)\n",
        "        opt.step()\n",
        "        total += float(loss.item())\n",
        "\n",
        "    tr_pred = predict_mlp(mlp, train_loader_eval, device_t)\n",
        "    va_pred = predict_mlp(mlp, val_loader_eval, device_t)\n",
        "\n",
        "    tr_rm, tr_ma = rmse(y_tr, tr_pred), mae(y_tr, tr_pred)\n",
        "    va_rm, va_ma = rmse(y_va, va_pred), mae(y_va, va_pred)\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": ep,\n",
        "        \"train/loss_epoch\": total / max(1, len(train_loader)),\n",
        "        \"train/rmse\": tr_rm, \"train/mae\": tr_ma,\n",
        "        \"val/rmse\": va_rm,   \"val/mae\": va_ma\n",
        "    })\n",
        "\n",
        "    print(f\"{MODEL_NAME} | Ep {ep:02d} | train RMSE {tr_rm:.4f} MAE {tr_ma:.4f} | val RMSE {va_rm:.4f} MAE {va_ma:.4f}\")\n",
        "\n",
        "    if va_rm < best_val:\n",
        "        best_val = va_rm\n",
        "        torch.save(mlp.state_dict(), best_path)\n",
        "        wandb.save(best_path)\n",
        "        print(\"saved best:\", best_path)\n",
        "\n",
        "train_time = time.time() - t1\n",
        "total_time = embed_time + train_time\n",
        "wandb.log({\"runtime_sec\": total_time, \"embed_time_sec\": embed_time, \"train_time_sec\": train_time})\n",
        "\n",
        "# TEST eval\n",
        "@torch.no_grad()\n",
        "def eval_on_testlabels_word_fe(best_path):\n",
        "    te_q = test_m[\"search_term\"].astype(str).tolist()\n",
        "    te_doc = (test_m[\"product_title\"].astype(str) + \" [SEP] \" + test_m[\"product_description\"].astype(str)).tolist()\n",
        "\n",
        "    qte_path = f\"/content/cache_{SAFE}_qte_len{MAX_LEN_Q}.npy\"\n",
        "    dte_path = f\"/content/cache_{SAFE}_dte_len{MAX_LEN_D}.npy\"\n",
        "\n",
        "    qte = encode_texts_cached(te_q,   max_len=MAX_LEN_Q, batch_size=EMBED_BS, cache_path=qte_path)\n",
        "    dte = encode_texts_cached(te_doc, max_len=MAX_LEN_D, batch_size=EMBED_BS, cache_path=dte_path)\n",
        "    Xte = build_pair_features(qte, dte)\n",
        "\n",
        "    mlp2 = MLPRegressor(Xte.shape[1], dropout=0.0).to(device_t)\n",
        "    mlp2.load_state_dict(torch.load(best_path, map_location=device_t))\n",
        "    mlp2.eval()\n",
        "\n",
        "    te_loader = DataLoader(FeatDataset(Xte, np.zeros(len(Xte), dtype=np.float32)),\n",
        "                           batch_size=1024, shuffle=False)\n",
        "    preds = predict_mlp(mlp2, te_loader, device_t)\n",
        "\n",
        "    pred_df = pd.DataFrame({\"id\": test_m[\"id\"].values, \"pred\": preds})\n",
        "    tl = test_labels.copy()\n",
        "    if \"Usage\" in tl.columns:\n",
        "        tl = tl[tl[\"Usage\"].isin([\"Public\", \"Private\"])].copy()\n",
        "\n",
        "    merged = tl.merge(pred_df, on=\"id\", how=\"inner\")\n",
        "    y_true = merged[\"relevance\"].astype(np.float32).values\n",
        "    y_pred = merged[\"pred\"].astype(np.float32).values\n",
        "    return rmse(y_true, y_pred), mae(y_true, y_pred), len(merged)\n",
        "\n",
        "\n",
        "# Final metrics (best checkpoint)\n",
        "mlp.load_state_dict(torch.load(best_path, map_location=device_t))\n",
        "\n",
        "tr_pred = predict_mlp(mlp, train_loader_eval, device_t)\n",
        "va_pred = predict_mlp(mlp, val_loader_eval, device_t)\n",
        "\n",
        "final_tr_rmse, final_tr_mae = rmse(y_tr, tr_pred), mae(y_tr, tr_pred)\n",
        "final_va_rmse, final_va_mae = rmse(y_va, va_pred), mae(y_va, va_pred)\n",
        "\n",
        "test_rmse, test_mae, n_test = eval_on_testlabels_word_fe(best_path)\n",
        "\n",
        "print(\"\\nFINAL BEST METRICS (WORD FE):\", MODEL_NAME)\n",
        "print(f\"Train | RMSE: {final_tr_rmse:.4f} | MAE: {final_tr_mae:.4f}\")\n",
        "print(f\"Val   | RMSE: {final_va_rmse:.4f} | MAE: {final_va_mae:.4f}\")\n",
        "print(f\"Test  | RMSE: {test_rmse:.4f} | MAE: {test_mae:.4f} | n={n_test}\")\n",
        "print(\"runtime_sec:\", int(total_time))\n",
        "print(\"best_path:\", best_path)\n",
        "\n",
        "wandb.log({\n",
        "    \"best/train_rmse\": final_tr_rmse,\n",
        "    \"best/train_mae\": final_tr_mae,\n",
        "    \"best/val_rmse\": final_va_rmse,\n",
        "    \"best/val_mae\": final_va_mae,\n",
        "    \"test/rmse\": test_rmse,\n",
        "    \"test/mae\": test_mae,\n",
        "    \"test/n_rows\": n_test,\n",
        "    \"best_path\": best_path\n",
        "})\n",
        "wandb.finish()\n",
        "\n",
        "# Row for Q3 table\n",
        "word_fe_row_1 = {\n",
        "    \"Model type\": f\"Word feature extractor: {MODEL_NAME}\",\n",
        "    \"runtime\": int(total_time),\n",
        "    \"Train RMSE\": float(final_tr_rmse),\n",
        "    \"Val-RMSE\": float(final_va_rmse),\n",
        "    \"Test-RMSE\": float(test_rmse),\n",
        "    \"Train MAE\": float(final_tr_mae),\n",
        "    \"Val-MAE\": float(final_va_mae),\n",
        "    \"Test-MAE\": float(test_mae),\n",
        "    \"best_path\": best_path,\n",
        "    \"MAX_LEN_Q\": MAX_LEN_Q,\n",
        "    \"MAX_LEN_D\": MAX_LEN_D\n",
        "}\n",
        "word_fe_row_1\n"
      ],
      "metadata": {
        "id": "PbAL63yf_5wU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}