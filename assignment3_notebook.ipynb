{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Part 1: Character-Level Siamese Models\n",
    "\n",
    "This notebook implements the end-to-end pipeline for the Home Depot Search Relevance assignment.",
    "It corresponds to the 5 steps outlined in the assignment pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing & Tokenization\n",
    "Handles data loading, cleaning, and character-level tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# --- Constants based on your statistics ---\n",
    "MAX_LEN_SEARCH = 64    # Covers Max (60)\n",
    "MAX_LEN_DESC = 2048    # Covers >95% (1842)\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# --- Paths ---\n",
    "DATA_DIR = 'data'\n",
    "OUTPUT_DIR = 'outputs'\n",
    "\n",
    "def load_and_merge():\n",
    "    print(\"Loading CSV files...\")\n",
    "    # Load with fallback encoding\n",
    "    try:\n",
    "        train = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'), encoding='ISO-8859-1')\n",
    "        test = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'), encoding='ISO-8859-1')\n",
    "        desc = pd.read_csv(os.path.join(DATA_DIR, 'product_descriptions.csv'), encoding='ISO-8859-1')\n",
    "    except:\n",
    "        train = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'), encoding='latin-1')\n",
    "        test = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'), encoding='latin-1')\n",
    "        desc = pd.read_csv(os.path.join(DATA_DIR, 'product_descriptions.csv'), encoding='latin-1')\n",
    "\n",
    "    # Merge descriptions\n",
    "    print(\"Merging descriptions...\")\n",
    "    train = pd.merge(train, desc, on='product_uid', how='left')\n",
    "    test = pd.merge(test, desc, on='product_uid', how='left')\n",
    "    \n",
    "    # Fill NaNs\n",
    "    train['search_term'] = train['search_term'].fillna(\"\")\n",
    "    train['product_description'] = train['product_description'].fillna(\"\")\n",
    "    train['product_title'] = train['product_title'].fillna(\"\")\n",
    "    \n",
    "    test['search_term'] = test['search_term'].fillna(\"\")\n",
    "    test['product_description'] = test['product_description'].fillna(\"\")\n",
    "    test['product_title'] = test['product_title'].fillna(\"\")\n",
    "    \n",
    "    # Create combined description (Title + Description) for better context\n",
    "    train['full_desc'] = train['product_title'] + \" \" + train['product_description']\n",
    "    test['full_desc'] = test['product_title'] + \" \" + test['product_description']\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "def build_char_dict(texts):\n",
    "    print(\"Building character dictionary...\")\n",
    "    chars = set()\n",
    "    for text in texts:\n",
    "        chars.update(text)\n",
    "    \n",
    "    # 0 is reserved for padding\n",
    "    char_to_int = {c: i + 1 for i, c in enumerate(sorted(list(chars)))}\n",
    "    return char_to_int\n",
    "\n",
    "def text_to_sequence(text, char_to_int, max_len):\n",
    "    seq = [char_to_int.get(c, 0) for c in text]\n",
    "    # Truncate\n",
    "    seq = seq[:max_len]\n",
    "    # Pad (Post-padding usually fine, or pre-padding)\n",
    "    # We will use zeros for padding\n",
    "    if len(seq) < max_len:\n",
    "        seq = seq + [0] * (max_len - len(seq))\n",
    "    return seq\n",
    "\n",
    "def prepare_dl_data(train_df, test_df):\n",
    "    print(\"\\n--- Preparing Data for Deep Learning (Character Sequences) ---\")\n",
    "    \n",
    "    # 1. Build Vocabulary from all text\n",
    "    all_text = pd.concat([\n",
    "        train_df['search_term'], \n",
    "        train_df['full_desc'],\n",
    "        test_df['search_term'],\n",
    "        test_df['full_desc']\n",
    "    ])\n",
    "    char_to_int = build_char_dict(all_text)\n",
    "    print(f\"Vocabulary Size: {len(char_to_int)} characters\")\n",
    "    \n",
    "    # 2. Convert to Sequences\n",
    "    print(\"Converting text to sequences (this may take a moment)...\")\n",
    "    \n",
    "    def process_column(series, max_len):\n",
    "        return np.array([text_to_sequence(t, char_to_int, max_len) for t in series], dtype=np.int8)\n",
    "    \n",
    "    X_train_search = process_column(train_df['search_term'], MAX_LEN_SEARCH)\n",
    "    X_train_desc = process_column(train_df['full_desc'], MAX_LEN_DESC)\n",
    "\n",
    "    # --- Save Tokenization Examples for Report ---\n",
    "    token_file = os.path.join(OUTPUT_DIR, 'tokenization_examples.txt')\n",
    "    print(f\"Saving tokenization examples to '{token_file}'...\")\n",
    "    with open(token_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"--- Tokenization Examples ---\\n\")\n",
    "        f.write(f\"Vocabulary Size: {len(char_to_int)}\\n\")\n",
    "        f.write(\"Char to Int Map (First 20): \" + str(list(char_to_int.items())[:20]) + \"...\\n\\n\")\n",
    "        \n",
    "        for i in range(5):\n",
    "            orig = train_df['search_term'].iloc[i]\n",
    "            seq = X_train_search[i]\n",
    "            # Convert non-zero seq back to readable for check (optional) \n",
    "            f.write(f\"Example {i+1}:\\n\")\n",
    "            f.write(f\"  Original: '{orig}'\\n\")\n",
    "            f.write(f\"  Sequence: {seq.tolist()[:20]} ... (truncated)\\n\\n\")\n",
    "    # ---------------------------------------------\n",
    "    \n",
    "    X_test_search = process_column(test_df['search_term'], MAX_LEN_SEARCH)\n",
    "    X_test_desc = process_column(test_df['full_desc'], MAX_LEN_DESC)\n",
    "    \n",
    "    y = train_df['relevance'].values\n",
    "    \n",
    "    # 3. Split Train into Train/Validation\n",
    "    print(\"Splitting Training data into Train/Validation (80/20)...\")\n",
    "    # We split indices to keep pairs together\n",
    "    indices = np.arange(len(y))\n",
    "    train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=RANDOM_SEED)\n",
    "    \n",
    "    dl_data = {\n",
    "        'X_train_search': X_train_search[train_idx],\n",
    "        'X_train_desc': X_train_desc[train_idx],\n",
    "        'y_train': y[train_idx],\n",
    "        \n",
    "        'X_val_search': X_train_search[val_idx],\n",
    "        'X_val_desc': X_train_desc[val_idx],\n",
    "        'y_val': y[val_idx],\n",
    "        \n",
    "        'X_test_search': X_test_search,\n",
    "        'X_test_desc': X_test_desc,\n",
    "        \n",
    "        'char_to_int': char_to_int,\n",
    "        'max_len_search': MAX_LEN_SEARCH,\n",
    "        'max_len_desc': MAX_LEN_DESC\n",
    "    }\n",
    "    \n",
    "    np.savez(os.path.join(DATA_DIR, 'dl_data.npz'), **dl_data)\n",
    "    print(\"Deep Learning data saved to 'dl_data.npz'\")\n",
    "\n",
    "def prepare_benchmark_data(train_df, test_df):\n",
    "    print(\"\\n--- Preparing Data for Benchmark (CountVectorizer n-gram 2,4) ---\")\n",
    "    \n",
    "    # Combine Search + Desc for Bag-of-Ngrams\n",
    "    # We add a separator\n",
    "    train_text = train_df['search_term'] + \" \\t \" + train_df['full_desc']\n",
    "    test_text = test_df['search_term'] + \" \\t \" + test_df['full_desc']\n",
    "    \n",
    "    # Setup Vectorizer (As requested: ngram 2-4)\n",
    "    # limit max_features to avoid OOM\n",
    "    print(\"Fitting CountVectorizer (char, ngram 2-4)...\")\n",
    "    vectorizer = CountVectorizer(\n",
    "        analyzer='char',\n",
    "        ngram_range=(2, 4),\n",
    "        min_df=5,       # Ignore very rare ngrams to save memory\n",
    "        dtype=np.uint16,\n",
    "        max_features=20000\n",
    "    )\n",
    "    \n",
    "    # Fit on Train, Transform both\n",
    "    # Note: Fitting on just train is standard practice to avoid data leakage\n",
    "    X_train_full = vectorizer.fit_transform(train_text)\n",
    "    X_test_full = vectorizer.transform(test_text)\n",
    "    \n",
    "    y = train_df['relevance'].values\n",
    "    \n",
    "    # Split\n",
    "    indices = np.arange(len(y))\n",
    "    train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=RANDOM_SEED)\n",
    "    \n",
    "    benchmark_data = {\n",
    "        'X_train': X_train_full[train_idx],\n",
    "        'y_train': y[train_idx],\n",
    "        'X_val': X_train_full[val_idx],\n",
    "        'y_val': y[val_idx],\n",
    "        'X_test': X_test_full,\n",
    "        'vectorizer': vectorizer\n",
    "    }\n",
    "    \n",
    "    joblib.dump(benchmark_data, os.path.join(DATA_DIR, 'benchmark_data.pkl'))\n",
    "    print(\"Benchmark data saved to 'benchmark_data.pkl'\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "train_df, test_df = load_and_merge()\n",
    "\n",
    "prepare_dl_data(train_df, test_df)\n",
    "prepare_benchmark_data(train_df, test_df)\n",
    "\n",
    "print(\"\\nPreprocessing Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Naive Benchmark Model\n",
    "Trains a Ridge Regression model on character n-grams (2-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- Constants ---\n",
    "RESULTS_FILE = 'results_log.csv'\n",
    "MODEL_FILE = 'outputs/benchmark_model.pkl'\n",
    "DATA_FILE = 'data/benchmark_data.pkl'\n",
    "SUBMISSION_FILE = 'outputs/submission_benchmark.npy'\n",
    "\n",
    "def evaluate(y_true, y_pred, name=\"Set\"):\n",
    "    y_pred = np.clip(y_pred, 1.0, 3.0)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    print(f\"  {name} RMSE: {rmse:.4f}\")\n",
    "    print(f\"  {name} MAE:  {mae:.4f}\")\n",
    "    return rmse, mae\n",
    "\n",
    "def log_results(model_name, runtime, train_rmse, val_rmse, train_mae, val_mae):\n",
    "    # Logs TRAIN/VAL results only. Test results handled in evaluate.py\n",
    "    file_exists = os.path.exists(RESULTS_FILE)\n",
    "    \n",
    "    # We leave Test columns empty or N/A for this script\n",
    "    df = pd.DataFrame([{\n",
    "        'Model type': model_name,\n",
    "        'runtime': f\"{runtime:.2f} sec\",\n",
    "        'Train RMSE': train_rmse,\n",
    "        'Val-RMSE': val_rmse,\n",
    "        'Test-RMSE': \"N/A (See evaluate.py)\",\n",
    "        'Train MAE': train_mae,\n",
    "        'Val-MAE': val_mae,\n",
    "        'Test-MAE': \"N/A (See evaluate.py)\"\n",
    "    }])\n",
    "    \n",
    "    if not file_exists:\n",
    "        df.to_csv(RESULTS_FILE, index=False)\n",
    "    else:\n",
    "        df.to_csv(RESULTS_FILE, mode='a', header=False, index=False)\n",
    "    \n",
    "    print(f\"\\nTraining Results saved to {RESULTS_FILE}\")\n",
    "\n",
    "def run_benchmark():\n",
    "    print(\"--- Training Benchmark Model (Ridge Regression) ---\")\n",
    "    \n",
    "    # 1. Load Data\n",
    "    if not os.path.exists(DATA_FILE):\n",
    "        print(f\"Error: {DATA_FILE} not found. Run step1_preprocess.py first.\")\n",
    "        return\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    data = joblib.load(DATA_FILE)\n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_val = data['X_val']\n",
    "    y_val = data['y_val']\n",
    "    X_test = data['X_test']\n",
    "    \n",
    "    # 2. Train Model\n",
    "    print(f\"Training Ridge Regression ({X_train.shape[0]} samples)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"Training completed in {train_time:.2f} seconds.\")\n",
    "    \n",
    "    # Save Model\n",
    "    joblib.dump(model, MODEL_FILE)\n",
    "    print(f\"Model saved to {MODEL_FILE}\")\n",
    "\n",
    "    # 3. Predict & Evaluate (Train/Val only)\n",
    "    print(\"\\nEvaluating on Train/Val...\")\n",
    "    \n",
    "    # Train\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    train_rmse, train_mae = evaluate(y_train, y_pred_train, \"Train\")\n",
    "    \n",
    "    # Val\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    val_rmse, val_mae = evaluate(y_val, y_pred_val, \"Val\")\n",
    "    \n",
    "    # Test (Prediction Only)\n",
    "    print(\"Generating Test predictions...\")\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    y_pred_test = np.clip(y_pred_test, 1.0, 3.0)\n",
    "    \n",
    "    # Save predictions\n",
    "    np.save(SUBMISSION_FILE, y_pred_test)\n",
    "    print(f\"Test predictions saved to '{SUBMISSION_FILE}'\")\n",
    "    \n",
    "    # 4. Log Results\n",
    "    log_results(\n",
    "        \"Naive Benchmark (Ridge Char 2-4gram)\", \n",
    "        train_time,\n",
    "        round(train_rmse, 4), \n",
    "        round(val_rmse, 4), \n",
    "        round(train_mae, 4), \n",
    "        round(val_mae, 4)\n",
    "    )\n",
    "\n",
    "# --- Main Execution ---\n",
    "run_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Siamese Char-CNN Training\n",
    "Trains the Siamese Network with Character-level CNN encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_FILE = 'data/dl_data.npz'\n",
    "RESULTS_FILE = 'results_log.csv'\n",
    "MODEL_SAVE_PATH = 'outputs/siamese_char_cnn.pt'\n",
    "SUBMISSION_FILE = 'outputs/submission_siamese_char.npy'\n",
    "PLOT_FILE = 'outputs/training_history_char_siamese.png'\n",
    "BATCH_SIZE = 64 \n",
    "EPOCHS = 15\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 256\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, search, desc, labels=None):\n",
    "        # We ensure they are the correct length and type for PyTorch\n",
    "        self.search = torch.tensor(search.astype(np.int64))\n",
    "        self.desc = torch.tensor(desc.astype(np.int64))\n",
    "        if labels is not None:\n",
    "            self.labels = torch.tensor(labels.astype(np.float32))\n",
    "        else:\n",
    "            self.labels = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.search)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.search[idx], self.desc[idx], self.labels[idx]\n",
    "        return self.search[idx], self.desc[idx]\n",
    "\n",
    "class CharCNNEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim):\n",
    "        super(CharCNNEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        \n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv1d(emb_dim, 128, kernel_size=7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(128, 256, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1) \n",
    "        )\n",
    "        self.fc = nn.Linear(256, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).transpose(1, 2) \n",
    "        x = self.convs(x).squeeze(-1) \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class SiameseCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, target_mean=2.38):\n",
    "        super(SiameseCNN, self).__init__()\n",
    "        self.encoder = CharCNNEncoder(vocab_size, emb_dim, hidden_dim)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            self.fc[-1].bias.fill_(target_mean)\n",
    "\n",
    "    def forward(self, s, d):\n",
    "        h1 = self.encoder(s)\n",
    "        h2 = self.encoder(d)\n",
    "        \n",
    "        diff = torch.abs(h1 - h2)\n",
    "        prod = h1 * h2\n",
    "        \n",
    "        combined = torch.cat([h1, h2, diff, prod], dim=1)\n",
    "        return self.fc(combined).squeeze(-1)\n",
    "\n",
    "def load_data():\n",
    "    print(\"Loading data...\")\n",
    "    if not os.path.exists(DATA_FILE):\n",
    "        raise FileNotFoundError(f\"{DATA_FILE} not found. Please run step1_preprocess.py first.\")\n",
    "        \n",
    "    with np.load(DATA_FILE, allow_pickle=True) as data:\n",
    "        char_to_int = data['char_to_int'].item()\n",
    "        vocab_size = len(char_to_int) + 1\n",
    "        return (data['X_train_search'], data['X_train_desc'], data['y_train'],\n",
    "                data['X_val_search'], data['X_val_desc'], data['y_val'],\n",
    "                data['X_test_search'], data['X_test_desc'], vocab_size)\n",
    "\n",
    "def log_results(runtime, train_rmse, val_rmse, train_mae, val_mae):\n",
    "    # Logs TRAIN/VAL results only.\n",
    "    res_df = pd.DataFrame([{\n",
    "        'Model type': 'Character level CNN (Siamese)',\n",
    "        'runtime': f\"{runtime:.2f} sec\",\n",
    "        'Train RMSE': f\"{train_rmse:.4f}\",\n",
    "        'Val-RMSE': f\"{val_rmse:.4f}\",\n",
    "        'Test-RMSE': \"N/A (See evaluate.py)\",\n",
    "        'Train MAE': f\"{train_mae:.4f}\",\n",
    "        'Val-MAE': f\"{val_mae:.4f}\",\n",
    "        'Test-MAE': \"N/A (See evaluate.py)\"\n",
    "    }])\n",
    "    res_df.to_csv(RESULTS_FILE, mode='a', header=not os.path.exists(RESULTS_FILE), index=False)\n",
    "    print(f\"\\nTraining Results saved to {RESULTS_FILE}\")\n",
    "\n",
    "def main():\n",
    "    # ... (existing device and loading code) ...\n",
    "    # ...\n",
    "    # After training loop:\n",
    "    # Load best model for final Train/Val/Test evaluation\n",
    "    print(\"\\nLoading best model for final evaluation...\")\n",
    "    model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "    \n",
    "    model.eval()\n",
    "    def get_metrics(loader):\n",
    "        preds, targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for s, d, y in loader:\n",
    "                s, d = s.to(device), d.to(device)\n",
    "                out = model(s, d)\n",
    "                preds.extend(out.cpu().numpy())\n",
    "                targets.extend(y.numpy())\n",
    "        preds = np.clip(preds, 1.0, 3.0)\n",
    "        rmse = np.sqrt(np.mean((np.array(targets) - np.array(preds))**2))\n",
    "        mae = np.mean(np.abs(np.array(targets) - np.array(preds)))\n",
    "        return rmse, mae\n",
    "\n",
    "    tr_rmse, tr_mae = get_metrics(DataLoader(CharDataset(X_s_tr, X_d_tr, y_tr), batch_size=BATCH_SIZE))\n",
    "    val_rmse, val_mae = get_metrics(DataLoader(CharDataset(X_s_val, X_d_val, y_val), batch_size=BATCH_SIZE))\n",
    "\n",
    "    # Test Prediction\n",
    "    test_loader = DataLoader(CharDataset(X_s_te, X_d_te, labels=None), batch_size=BATCH_SIZE)\n",
    "    # ... (rest of the test pred logic) ...\n",
    "    \n",
    "    # Log Results\n",
    "    log_results(total_time, tr_rmse, val_rmse, tr_mae, val_mae)\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Extraction & Machine Learning\n",
    "Extracts embeddings from the trained CNN and trains XGBoost/Ridge models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_FILE = 'data/dl_data.npz'\n",
    "MODEL_PATH = 'outputs/siamese_char_cnn.pt'\n",
    "RESULTS_FILE = 'results_log.csv'\n",
    "SUBMISSION_XGB = 'outputs/submission_char_fe_xgb.npy'\n",
    "SUBMISSION_RIDGE = 'outputs/submission_char_fe_ridge.npy'\n",
    "BATCH_SIZE = 128\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "# --- Re-define Model Classes (Must match training) ---\n",
    "class CharCNNEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim):\n",
    "        super(CharCNNEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv1d(emb_dim, 128, kernel_size=7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(128, 256, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1) \n",
    "        )\n",
    "        self.fc = nn.Linear(256, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).transpose(1, 2) \n",
    "        x = self.convs(x).squeeze(-1) \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class SiameseCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim):\n",
    "        super(SiameseCNN, self).__init__()\n",
    "        self.encoder = CharCNNEncoder(vocab_size, emb_dim, hidden_dim)\n",
    "        # We don't need the rest for feature extraction\n",
    "\n",
    "    def forward(self, s, d):\n",
    "        return self.encoder(s), self.encoder(d)\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, search, desc):\n",
    "        self.search = torch.tensor(search.astype(np.int64))\n",
    "        self.desc = torch.tensor(desc.astype(np.int64))\n",
    "    def __len__(self): return len(self.search)\n",
    "    def __getitem__(self, idx): return self.search[idx], self.desc[idx]\n",
    "\n",
    "def extract_features(model, loader, device):\n",
    "    model.eval()\n",
    "    feats = []\n",
    "    with torch.no_grad():\n",
    "        for s, d in loader:\n",
    "            s, d = s.to(device), d.to(device)\n",
    "            h1, h2 = model(s, d)\n",
    "            \n",
    "            h1 = h1.cpu().numpy()\n",
    "            h2 = h2.cpu().numpy()\n",
    "            \n",
    "            # Create Interaction Features\n",
    "            diff = np.abs(h1 - h2)\n",
    "            prod = h1 * h2\n",
    "            cosine = np.sum(h1 * h2, axis=1, keepdims=True) / (\n",
    "                np.linalg.norm(h1, axis=1, keepdims=True) * np.linalg.norm(h2, axis=1, keepdims=True) + 1e-8\n",
    "            )\n",
    "            euclid = np.linalg.norm(h1 - h2, axis=1, keepdims=True)\n",
    "            \n",
    "            # Concatenate all\n",
    "            batch_feats = np.hstack([h1, h2, diff, prod, cosine, euclid])\n",
    "            feats.append(batch_feats)\n",
    "            \n",
    "    return np.vstack(feats)\n",
    "\n",
    "def log_results(model_name, runtime, train_rmse, val_rmse, train_mae, val_mae):\n",
    "    print(f\"Logging {model_name}...\")\n",
    "    res_df = pd.DataFrame([{\n",
    "        'Model type': model_name,\n",
    "        'runtime': f\"{runtime:.2f} sec\",\n",
    "        'Train RMSE': f\"{train_rmse:.4f}\",\n",
    "        'Val-RMSE': f\"{val_rmse:.4f}\",\n",
    "        'Test-RMSE': \"N/A (See evaluate.py)\",\n",
    "        'Train MAE': f\"{train_mae:.4f}\",\n",
    "        'Val-MAE': f\"{val_mae:.4f}\",\n",
    "        'Test-MAE': \"N/A (See evaluate.py)\"\n",
    "    }])\n",
    "    res_df.to_csv(RESULTS_FILE, mode='a', header=not os.path.exists(RESULTS_FILE), index=False)\n",
    "\n",
    "def main():\n",
    "    # ... (existing loading and feature extraction code) ...\n",
    "\n",
    "    # 4. Train Model 1: XGBoost on Char Features...\n",
    "    print(\"\\nTraining Model 1: XGBoost on Char Features...\")\n",
    "    start = time.time()\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=100, \n",
    "        max_depth=6, \n",
    "        learning_rate=0.1, \n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    xgb_model.fit(X_train_feats, y_train)\n",
    "    rt = time.time() - start\n",
    "    \n",
    "    # Eval XGB\n",
    "    tr_pred = np.clip(xgb_model.predict(X_train_feats), 1.0, 3.0)\n",
    "    val_pred = np.clip(xgb_model.predict(X_val_feats), 1.0, 3.0)\n",
    "    tr_rmse = np.sqrt(mean_squared_error(y_train, tr_pred))\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    tr_mae = mean_absolute_error(y_train, tr_pred)\n",
    "    val_mae = mean_absolute_error(y_val, val_pred)\n",
    "    \n",
    "    print(f\"XGB Results - Train RMSE: {tr_rmse:.4f}, Val RMSE: {val_rmse:.4f}\")\n",
    "    log_results(\"FE (Char) + XGBoost\", rt, tr_rmse, val_rmse, tr_mae, val_mae)\n",
    "    \n",
    "    # Save XGB Preds\n",
    "    te_pred = xgb_model.predict(X_test_feats)\n",
    "    np.save(SUBMISSION_XGB, te_pred)\n",
    "    \n",
    "    # 5. Train Model 2: Ridge on Char Features...\n",
    "    print(\"\\nTraining Model 2: Ridge on Char Features...\")\n",
    "    start = time.time()\n",
    "    ridge_model = Ridge(alpha=1.0)\n",
    "    ridge_model.fit(X_train_feats, y_train)\n",
    "    rt = time.time() - start\n",
    "    \n",
    "    # Eval Ridge\n",
    "    tr_pred = np.clip(ridge_model.predict(X_train_feats), 1.0, 3.0)\n",
    "    val_pred = np.clip(ridge_model.predict(X_val_feats), 1.0, 3.0)\n",
    "    tr_rmse = np.sqrt(mean_squared_error(y_train, tr_pred))\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    tr_mae = mean_absolute_error(y_train, tr_pred)\n",
    "    val_mae = mean_absolute_error(y_val, val_pred)\n",
    "    \n",
    "    print(f\"Ridge Results - Train RMSE: {tr_rmse:.4f}, Val RMSE: {val_rmse:.4f}\")\n",
    "    log_results(\"FE (Char) + Ridge\", rt, tr_rmse, val_rmse, tr_mae, val_mae)\n",
    "\n",
    "    \n",
    "    # Save Ridge Preds\n",
    "    te_pred = ridge_model.predict(X_test_feats)\n",
    "    np.save(SUBMISSION_RIDGE, te_pred)\n",
    "    \n",
    "    print(\"\\nDone with Char Feature Extraction!\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Final Evaluation\n",
    "Evaluates all models against the solution file and generates the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# --- Configuration ---\n",
    "SOLUTION_FILE = 'data/solution.csv'\n",
    "RESULTS_FILE = 'results_log.csv'\n",
    "PRED_FILES = {\n",
    "    'Benchmark': 'outputs/submission_benchmark.npy',\n",
    "    'Siamese Char CNN': 'outputs/submission_siamese_char.npy',\n",
    "    'FE (Char) + XGBoost': 'outputs/submission_char_fe_xgb.npy',\n",
    "    'FE (Char) + Ridge': 'outputs/submission_char_fe_ridge.npy'\n",
    "}\n",
    "\n",
    "def load_solution():\n",
    "    if not os.path.exists(SOLUTION_FILE):\n",
    "        print(f\"Error: {SOLUTION_FILE} not found.\")\n",
    "        return None, None\n",
    "    \n",
    "    df = pd.read_csv(SOLUTION_FILE)\n",
    "    \n",
    "    # Filter Usage\n",
    "    if 'Usage' in df.columns:\n",
    "        # Keep Public and Private, ignore Ignored\n",
    "        # The user said \"remove -1\", which usually corresponds to Ignored.\n",
    "        mask = df['Usage'] != 'Ignored'\n",
    "        # Double check with relevance just in case\n",
    "        if 'relevance' in df.columns:\n",
    "             mask = mask & (df['relevance'] != -1)\n",
    "        \n",
    "        filtered_df = df[mask]\n",
    "        valid_indices = df.index[mask].to_numpy() # Original indices to slice predictions\n",
    "        y_true = filtered_df['relevance'].values\n",
    "        return y_true, valid_indices\n",
    "    elif 'relevance' in df.columns:\n",
    "        # Fallback if Usage column missing but -1 exists\n",
    "        mask = df['relevance'] != -1\n",
    "        filtered_df = df[mask]\n",
    "        valid_indices = df.index[mask].to_numpy()\n",
    "        y_true = filtered_df['relevance'].values\n",
    "        return y_true, valid_indices\n",
    "        \n",
    "    return None, None\n",
    "\n",
    "def evaluate_predictions(name, pred_file, y_true, valid_indices):\n",
    "    if not os.path.exists(pred_file):\n",
    "        print(f\"[{name}] Prediction file {pred_file} not found.\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        y_pred_full = np.load(pred_file)\n",
    "        \n",
    "        # Check length\n",
    "        # We assume y_pred_full corresponds to the full test/solution file\n",
    "        # If lengths match full solution, we slice.\n",
    "        # Note: valid_indices corresponds to the row number in solution.csv (0-based)\n",
    "        \n",
    "        if len(y_pred_full) < np.max(valid_indices):\n",
    "             print(f\"[{name}] Warning: Prediction length {len(y_pred_full)} < Max Index {np.max(valid_indices)}.\")\n",
    "             return None, None\n",
    "             \n",
    "        y_pred = y_pred_full[valid_indices]\n",
    "        \n",
    "        # Clip just in case\n",
    "        y_pred = np.clip(y_pred, 1.0, 3.0)\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        \n",
    "        print(f\"--- {name} ---\")\n",
    "        print(f\"  RMSE: {rmse:.4f}\")\n",
    "        print(f\"  MAE:  {mae:.4f}\")\n",
    "        \n",
    "        return rmse, mae\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {name}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def update_log(name, rmse, mae):\n",
    "    if not os.path.exists(RESULTS_FILE):\n",
    "        print(\"Results log not found, cannot update.\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(RESULTS_FILE)\n",
    "    \n",
    "    # Mapping display names to Model type names in the CSV\n",
    "    name_map = {\n",
    "        'Benchmark': 'Naive Benchmark',\n",
    "        'Siamese Char CNN': 'Character level CNN',\n",
    "        'FE (Char) + XGBoost': 'FE (Char) + XGBoost',\n",
    "        'FE (Char) + Ridge': 'FE (Char) + Ridge'\n",
    "    }\n",
    "    \n",
    "    csv_name = name_map.get(name, name)\n",
    "    \n",
    "    updated = False\n",
    "    for i in range(len(df)):\n",
    "        if csv_name in str(df.loc[i, 'Model type']):\n",
    "            # If we find a row for this model, we update it.\n",
    "            # We prefer updating rows that have \"N/A\" for Test metrics.\n",
    "            if df.loc[i, 'Test-RMSE'] == \"N/A (See evaluate.py)\" or pd.isna(df.loc[i, 'Test-RMSE']):\n",
    "                df.loc[i, 'Test-RMSE'] = round(rmse, 4)\n",
    "                df.loc[i, 'Test-MAE'] = round(mae, 4)\n",
    "                updated = True\n",
    "                print(f\"Updated existing entry for {df.loc[i, 'Model type']}\")\n",
    "                break\n",
    "\n",
    "    if not updated:\n",
    "        # Check if any row matches exactly, even if it's already filled\n",
    "        for i in range(len(df)):\n",
    "            if csv_name == str(df.loc[i, 'Model type']):\n",
    "                df.loc[i, 'Test-RMSE'] = round(rmse, 4)\n",
    "                df.loc[i, 'Test-MAE'] = round(mae, 4)\n",
    "                updated = True\n",
    "                print(f\"Overwrote existing entry for {df.loc[i, 'Model type']}\")\n",
    "                break\n",
    "    \n",
    "    if not updated:\n",
    "        print(f\"Could not find entry for {name} in {RESULTS_FILE}. Appending new row.\")\n",
    "        new_row = {\n",
    "            'Model type': csv_name,\n",
    "            'runtime': \"-\",\n",
    "            'Train RMSE': \"-\",\n",
    "            'Val-RMSE': \"-\",\n",
    "            'Test-RMSE': round(rmse, 4),\n",
    "            'Train MAE': \"-\",\n",
    "            'Val-MAE': \"-\",\n",
    "            'Test-MAE': round(mae, 4)\n",
    "        }\n",
    "        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    df.to_csv(RESULTS_FILE, index=False)\n",
    "\n",
    "def main():\n",
    "    print(\"Loading Ground Truth from solution.csv...\")\n",
    "    y_true, valid_indices = load_solution()\n",
    "    \n",
    "    if y_true is None:\n",
    "        print(\"Could not load valid labels.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(y_true)} valid test samples (filtered 'Ignored/-1').\")\n",
    "    \n",
    "    for name, pred_file in PRED_FILES.items():\n",
    "        rmse, mae = evaluate_predictions(name, pred_file, y_true, valid_indices)\n",
    "        if rmse is not None:\n",
    "            update_log(name, rmse, mae)\n",
    "\n",
    "    print(f\"\\nEvaluation Complete. Updated {RESULTS_FILE}.\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}